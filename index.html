<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Oylwhu Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Oylwhu Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Oylwhu Blog">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Oylwhu Blog">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Oylwhu Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xract.com1.z0.glb.clouddn.com/favicon.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Oylwhu Blog</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/oylwhu" title="github">github</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/network/" style="font-size: 15px;">network</a> <a href="/tags/学习笔记/" style="font-size: 20px;">学习笔记</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">短暂年华，绚丽人生。用那个青春的热血点燃未来的希望，让人生的每一个角落都色彩飞扬。让那犹如白纸的未来色彩斑斓，不再单调，让我们的人生像一部电影不断演绎精彩的每一个片段。</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Oylwhu Blog</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="http://7xract.com1.z0.glb.clouddn.com/favicon.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Oylwhu Blog</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/oylwhu" title="github">github</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Job的MapReduce Task初始化" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/10/12/Job的MapReduce Task初始化/" class="article-date">
  	<time datetime="2015-10-12T12:32:10.000Z" itemprop="datePublished">2015-10-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/12/Job的MapReduce Task初始化/">Job的Map/Reduce Task初始化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Job_u7684Map/Reduce_Task_u521D_u59CB_u5316"><a href="#Job_u7684Map/Reduce_Task_u521D_u59CB_u5316" class="headerlink" title="Job的Map/Reduce Task初始化"></a>Job的Map/Reduce Task初始化</h2><p>【转】<a href="http://www.cnblogs.com/esingchan/p/3917252.html">原文</a><br>上一节分析了Job由JobClient提交到JobTracker的流程，利用RPC机制，JobTracker接收到Job ID和Job所在HDFS的目录，够早了JobInProgress对象，丢入队列，另一个线程从队列中取出JobInProgress对象，并丢入线程池中执行，执行JobInProgress的initJob方法，我们逐步分析。</p>
<pre><code>public void initJob(JobInProgress job) {
  if (null == job) {
    LOG.info(&quot;Init on null job is not valid&quot;);
    return;
  }

  try {
    JobStatus prevStatus = (JobStatus)job.getStatus().clone();
    LOG.info(&quot;Initializing &quot; + job.getJobID());
    job.initTasks();
    // Inform the listeners if the job state has changed
    // Note : that the job will be in PREP state.
    JobStatus newStatus = (JobStatus)job.getStatus().clone();
    if (prevStatus.getRunState() != newStatus.getRunState()) {
      JobStatusChangeEvent event = 
        new JobStatusChangeEvent(job, EventType.RUN_STATE_CHANGED, prevStatus, 
            newStatus);
      synchronized (JobTracker.this) {
        updateJobInProgressListeners(event);
      }
    }
  } catch (KillInterruptedException kie) {
    //   If job was killed during initialization, job state will be KILLED
    LOG.error(&quot;Job initialization interrupted:\n&quot; +
        StringUtils.stringifyException(kie));
    killJob(job);
  } catch (Throwable t) {
    String failureInfo = 
      &quot;Job initialization failed:\n&quot; + StringUtils.stringifyException(t);
    // If the job initialization is failed, job state will be FAILED
    LOG.error(failureInfo);
    job.getStatus().setFailureInfo(failureInfo);
    failJob(job);
  }
}
</code></pre><p>可以看出，先进行 job.initTasks()，初始化Map和Reduce任务，之后更新所有</p>
<pre><code>synchronized (JobTracker.this) {
  updateJobInProgressListeners(event);
}
</code></pre><p>Map/Reduce Task初始化完毕是一个事件，下面的代码进行消息通知：</p>
<pre><code>// Update the listeners about the job
// Assuming JobTracker is locked on entry.
private void updateJobInProgressListeners(JobChangeEvent event) {
  for (JobInProgressListener listener : jobInProgressListeners) {
    listener.jobUpdated(event);
  }
}
</code></pre><p>可见，在Job放入队列时使用的是jobAdded，此时使用的是jobUpdated。我们在后面再分析jobUpdated后的细节，此时先分析从jobAdded到jobUpdated之间，Job的初始化过程，主要分为几个阶段。</p>
<p>首先执行的是获取Split信息，这一部分信息事先已经由JobClient上传至HDFS中。</p>
<hr>
<p>1、读取Split信息：</p>
<pre><code>//
// read input splits and create a map per a split
//
TaskSplitMetaInfo[] splits = createSplits(jobId);
if (numMapTasks != splits.length) {
  throw new IOException(&quot;Number of maps in JobConf doesn&apos;t match number of &quot; +
      &quot;recieved splits for job &quot; + jobId + &quot;! &quot; +
      &quot;numMapTasks=&quot; + numMapTasks + &quot;, #splits=&quot; + splits.length);
}
numMapTasks = splits.length;
</code></pre><p>createSplits方法的代码为：</p>
<pre><code>TaskSplitMetaInfo[] createSplits(org.apache.hadoop.mapreduce.JobID jobId)
throws IOException {
  TaskSplitMetaInfo[] allTaskSplitMetaInfo =
    SplitMetaInfoReader.readSplitMetaInfo(jobId, fs, jobtracker.getConf(),
        jobSubmitDir);
  return allTaskSplitMetaInfo;
}
</code></pre><p>即读取job.splitmetainfo文件，获得Split信息：</p>
<pre><code>public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(
    JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) 
throws IOException {
  long maxMetaInfoSize = conf.getLong(&quot;mapreduce.jobtracker.split.metainfo.maxsize&quot;, 
      10000000L);
  Path metaSplitFile = JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);
  FileStatus fStatus = fs.getFileStatus(metaSplitFile);
  if (maxMetaInfoSize &gt; 0 &amp;&amp; fStatus.getLen() &gt; maxMetaInfoSize) {
    throw new IOException(&quot;Split metadata size exceeded &quot; +
        maxMetaInfoSize +&quot;. Aborting job &quot; + jobId);
  }
  FSDataInputStream in = fs.open(metaSplitFile);
  byte[] header = new byte[JobSplit.META_SPLIT_FILE_HEADER.length];
  in.readFully(header);
  if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {
    throw new IOException(&quot;Invalid header on split file&quot;);
  }
  int vers = WritableUtils.readVInt(in);
  if (vers != JobSplit.META_SPLIT_VERSION) {
    in.close();
    throw new IOException(&quot;Unsupported split version &quot; + vers);
  }
  int numSplits = WritableUtils.readVInt(in); //TODO: check for insane values
  JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo = 
    new JobSplit.TaskSplitMetaInfo[numSplits];
  final int maxLocations =
    conf.getInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, Integer.MAX_VALUE);
  for (int i = 0; i &lt; numSplits; i++) {
    JobSplit.SplitMetaInfo splitMetaInfo = new JobSplit.SplitMetaInfo();
    splitMetaInfo.readFields(in);
    final int numLocations = splitMetaInfo.getLocations().length;
    if (numLocations &gt; maxLocations) {
      throw new IOException(&quot;Max block location exceeded for split: #&quot;  + i +
            &quot; splitsize: &quot; + numLocations + &quot; maxsize: &quot; + maxLocations);
    }
    JobSplit.TaskSplitIndex splitIndex = new JobSplit.TaskSplitIndex(
        JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), 
        splitMetaInfo.getStartOffset());
    allSplitMetaInfo[i] = new JobSplit.TaskSplitMetaInfo(splitIndex, 
        splitMetaInfo.getLocations(), 
        splitMetaInfo.getInputDataLength());
  }
  in.close();
  return allSplitMetaInfo;
}
</code></pre><p>涉及读取文件的代码有：</p>
<pre><code>FSDataInputStream in = fs.open(metaSplitFile);
byte[] header = new byte[JobSplit.META_SPLIT_FILE_HEADER.length];
in.readFully(header);
</code></pre><p>这一部分先读取job.splitmetainfo文件的头部，头部实际上是字符串”META-SPL“，该信息由下面的类指定：</p>
<pre><code>public class JobSplit {
  static final int META_SPLIT_VERSION = 1;
  static final byte[] META_SPLIT_FILE_HEADER;

  static {
    try {
      META_SPLIT_FILE_HEADER = &quot;META-SPL&quot;.getBytes(&quot;UTF-8&quot;);
    } catch (UnsupportedEncodingException u) {
      throw new RuntimeException(u);
    }
  } 
.......
</code></pre><p>读取了文件头之后，剩下的是读取版本信息：</p>
<pre><code>int vers = WritableUtils.readVInt(in);
if (vers != JobSplit.META_SPLIT_VERSION) {
  in.close();
  throw new IOException(&quot;Unsupported split version &quot; + vers);
}
</code></pre><p>检查了版本（1）后，接下来就是读取Split的数量：</p>
<pre><code>int numSplits = WritableUtils.readVInt(in); //TODO: check for insane values
JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo = 
  new JobSplit.TaskSplitMetaInfo[numSplits];
</code></pre><p>并根据Split数量创建JobSplit.TaskSplitMetaInfo数组。接下来对于每个Split，循环读取位置等信息：</p>
<pre><code>for (int i = 0; i &lt; numSplits; i++) {
  JobSplit.SplitMetaInfo splitMetaInfo = new JobSplit.SplitMetaInfo();
  splitMetaInfo.readFields(in);
  final int numLocations = splitMetaInfo.getLocations().length;
  if (numLocations &gt; maxLocations) {
    throw new IOException(&quot;Max block location exceeded for split: #&quot;  + i +
          &quot; splitsize: &quot; + numLocations + &quot; maxsize: &quot; + maxLocations);
  }
  JobSplit.TaskSplitIndex splitIndex = new JobSplit.TaskSplitIndex(
      JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), 
      splitMetaInfo.getStartOffset());
  allSplitMetaInfo[i] = new JobSplit.TaskSplitMetaInfo(splitIndex, 
      splitMetaInfo.getLocations(), 
      splitMetaInfo.getInputDataLength());
}
</code></pre><p>在上面的代码中，splitMetaInfo.readFields(in)可以获得位置信息：</p>
<pre><code>public void readFields(DataInput in) throws IOException {
  int len = WritableUtils.readVInt(in);
  locations = new String[len];
  for (int i = 0; i &lt; locations.length; i++) {
    locations[i] = Text.readString(in);
  }
  startOffset = WritableUtils.readVLong(in);
  inputDataLength = WritableUtils.readVLong(in);
}
</code></pre><p>所谓的位置，实际上就是指这个Split在j哪些服务器的信息。获取到位置、Split数据长度等信息后，全部纪录在对象JobSplit.TaskSplitMetaInfo中：</p>
<pre><code>JobSplit.TaskSplitIndex splitIndex = new JobSplit.TaskSplitIndex(
    JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), 
    splitMetaInfo.getStartOffset());
allSplitMetaInfo[i] = new JobSplit.TaskSplitMetaInfo(splitIndex, 
    splitMetaInfo.getLocations(), 
    splitMetaInfo.getInputDataLength());
</code></pre><p>返回allSplitMetaInfo数组。</p>
<hr>
<p>2、根据Map任务数量创建相同数量的TaskInProgress对象：<br>上面返回的数组大小即纪录了Split的个数，也决定了Map的数量，验证这些服务器的合法性：</p>
<pre><code>numMapTasks = splits.length;
// Sanity check the locations so we don&apos;t create/initialize unnecessary tasks
for (TaskSplitMetaInfo split : splits) {
  NetUtils.verifyHostnames(split.getLocations());
}
</code></pre><p>在监控相关类中设置相应信息：</p>
<pre><code>jobtracker.getInstrumentation().addWaitingMaps(getJobID(), numMapTasks);
jobtracker.getInstrumentation().addWaitingReduces(getJobID(), numReduceTasks);
this.queueMetrics.addWaitingMaps(getJobID(), numMapTasks);
this.queueMetrics.addWaitingReduces(getJobID(), numReduceTasks);
</code></pre><p>接下来创建TaskInProgress对象，每个Map都对应于一个TaskInProgress对象：</p>
<pre><code>maps = new TaskInProgress[numMapTasks];
for(int i=0; i &lt; numMapTasks; ++i) {
  inputLength += splits[i].getInputDataLength();
  maps[i] = new TaskInProgress(jobId, jobFile, 
                               splits[i], 
                               jobtracker, conf, this, i, numSlotsPerMap);
}
</code></pre><p>TaskInProgress纪录了一个Map Task或Reduce Task运行相关的所有信息，类似于JobInProgress，TaskInProgress的构造函数有两个，分别针对Map和Reduce的，对于Map的：</p>
<pre><code>/**
 * Constructor for MapTask
 */
public TaskInProgress(JobID jobid, String jobFile, 
                      TaskSplitMetaInfo split, 
                      JobTracker jobtracker, JobConf conf, 
                      JobInProgress job, int partition,
                      int numSlotsRequired) {
  this.jobFile = jobFile;
  this.splitInfo = split;
  this.jobtracker = jobtracker;
  this.job = job;
  this.conf = conf;
  this.partition = partition;
  this.maxSkipRecords = SkipBadRecords.getMapperMaxSkipRecords(conf);
  this.numSlotsRequired = numSlotsRequired;
  setMaxTaskAttempts();
  init(jobid);
}
</code></pre><p>splitInfo纪录了当前Split的信息，partition即表示这是第几个Map Task，numSlotsRequired为1.<br>创建好的TaskInProgress将会放入缓存中：</p>
<pre><code>if (numMapTasks &gt; 0) { 
  nonRunningMapCache = createCache(splits, maxLevel);
}
</code></pre><p>nonRunningMapCache是一个未运行起来的Map任务的关于主机信息等等的缓存，其索引为Node，即服务器；而其值为TaskInProgress对象，其声明为，因此，实际上就是解析Split所在的服务器，缓存下来，供后续调度使用：</p>
<pre><code>Map&lt;Node, List&lt;TaskInProgress&gt;&gt; nonRunningMapCache;
</code></pre><p>其方法代码为：</p>
<pre><code>private Map&lt;Node, List&lt;TaskInProgress&gt;&gt; createCache(
                               TaskSplitMetaInfo[] splits, int maxLevel)
                               throws UnknownHostException {
  Map&lt;Node, List&lt;TaskInProgress&gt;&gt; cache = 
    new IdentityHashMap&lt;Node, List&lt;TaskInProgress&gt;&gt;(maxLevel);

  Set&lt;String&gt; uniqueHosts = new TreeSet&lt;String&gt;();
  for (int i = 0; i &lt; splits.length; i++) {
    String[] splitLocations = splits[i].getLocations();
    if (splitLocations == null || splitLocations.length == 0) {
      nonLocalMaps.add(maps[i]);
      continue;
    }
    for(String host: splitLocations) {
      Node node = jobtracker.resolveAndAddToTopology(host);
      uniqueHosts.add(host);
      LOG.info(&quot;tip:&quot; + maps[i].getTIPId() + &quot; has split on node:&quot; + node);
      for (int j = 0; j &lt; maxLevel; j++) {
        List&lt;TaskInProgress&gt; hostMaps = cache.get(node);
        if (hostMaps == null) {
          hostMaps = new ArrayList&lt;TaskInProgress&gt;();
          cache.put(node, hostMaps);
          hostMaps.add(maps[i]);
        }
        //check whether the hostMaps already contains an entry for a TIP
        //This will be true for nodes that are racks and multiple nodes in
        //the rack contain the input for a tip. Note that if it already
        //exists in the hostMaps, it must be the last element there since
        //we process one TIP at a time sequentially in the split-size order
        if (hostMaps.get(hostMaps.size() - 1) != maps[i]) {
          hostMaps.add(maps[i]);
        }
        node = node.getParent();
      }
    }
  }

  // Calibrate the localityWaitFactor - Do not override user intent!
  if (localityWaitFactor == DEFAULT_LOCALITY_WAIT_FACTOR) {
    int jobNodes = uniqueHosts.size();
    int clusterNodes = jobtracker.getNumberOfUniqueHosts();

    if (clusterNodes &gt; 0) {
      localityWaitFactor = 
        Math.min((float)jobNodes/clusterNodes, localityWaitFactor);
    }
    LOG.info(jobId + &quot; LOCALITY_WAIT_FACTOR=&quot; + localityWaitFactor);
  }

  return cache;
}
</code></pre><hr>
<p>3、根据Reduce任务数量创建相同数量的TaskInProgress对象：<br>代码和Map基本相同：</p>
<pre><code>//
// Create reduce tasks
//
this.reduces = new TaskInProgress[numReduceTasks];
for (int i = 0; i &lt; numReduceTasks; i++) {
  reduces[i] = new TaskInProgress(jobId, jobFile, 
                                  numMapTasks, i, 
                                  jobtracker, conf, this, numSlotsPerReduce);
  nonRunningReduces.add(reduces[i]);
}
</code></pre><hr>
<p> 4、计算Reduce任务启动前Map最少应该启动的数量：<br>根据MapReduce原理，先进行Map计算，之后中间结果再传递至Reduce计算，因此，Map要先进行计算，Reduce如果和Map一起启动，那么，Reduce必然先一直处于等待中。这会消耗机器资源，且Shuffle时间比较长。所以，这个值默认是Map所有任务数量的5%：</p>
<pre><code>// Calculate the minimum number of maps to be complete before 
// we should start scheduling reduces
completedMapsForReduceSlowstart = 
  (int)Math.ceil(
      (conf.getFloat(&quot;mapred.reduce.slowstart.completed.maps&quot;, 
                     DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART) * 
       numMapTasks));
// ... use the same for estimating the total output of all maps
resourceEstimator.setThreshhold(completedMapsForReduceSlowstart);
</code></pre><p>从DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART可以看出，是5%:</p>
<pre><code>private static float DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART = 0.05f;
</code></pre><hr>
<p> 5、创建Map和Reduce任务的清理任务，各一个：</p>
<pre><code>// create cleanup two cleanup tips, one map and one reduce.
cleanup = new TaskInProgress[2];
// cleanup map tip. This map doesn&apos;t use any splits. Just assign an empty
// split.
TaskSplitMetaInfo emptySplit = JobSplit.EMPTY_TASK_SPLIT;
cleanup[0] = new TaskInProgress(jobId, jobFile, emptySplit, 
        jobtracker, conf, this, numMapTasks, 1);
cleanup[0].setJobCleanupTask();
// cleanup reduce tip.
cleanup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
                   numReduceTasks, jobtracker, conf, this, 1);
cleanup[1].setJobCleanupTask();
</code></pre><hr>
<p> 6、创建Map和Reduce任务的启动任务，各一个：</p>
<pre><code>// create two setup tips, one map and one reduce.
setup = new TaskInProgress[2];

// setup map tip. This map doesn&apos;t use any split. Just assign an empty
// split.
setup[0] = new TaskInProgress(jobId, jobFile, emptySplit, 
        jobtracker, conf, this, numMapTasks + 1, 1);
setup[0].setJobSetupTask();

// setup reduce tip.
setup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
                   numReduceTasks + 1, jobtracker, conf, this, 1);
setup[1].setJobSetupTask();
</code></pre><hr>
<p> 7、Map/Reduce Task初始化完毕：</p>
<pre><code>synchronized(jobInitKillStatus){
  jobInitKillStatus.initDone = true;

  // set this before the throw to make sure cleanup works properly
  tasksInited = true;

  if(jobInitKillStatus.killed) {
    throw new KillInterruptedException(&quot;Job &quot; + jobId + &quot; killed in init&quot;);
  }
}
</code></pre><p>初始化完毕后，会通过jobUpdated进行通知。Job更新的事件主要有三种：</p>
<pre><code>static enum EventType {RUN_STATE_CHANGED, START_TIME_CHANGED, PRIORITY_CHANGED}
</code></pre><p>此时初始化完毕属于RUN_STATE_CHANGED。从其代码来看，如果是运行状态改变，并不执行什么操作：</p>
<pre><code>public synchronized void jobUpdated(JobChangeEvent event) {
  JobInProgress job = event.getJobInProgress();
  if (event instanceof JobStatusChangeEvent) {
    // Check if the ordering of the job has changed
    // For now priority and start-time can change the job ordering
    JobStatusChangeEvent statusEvent = (JobStatusChangeEvent)event;
    JobSchedulingInfo oldInfo =  
      new JobSchedulingInfo(statusEvent.getOldStatus());
    if (statusEvent.getEventType() == EventType.PRIORITY_CHANGED 
        || statusEvent.getEventType() == EventType.START_TIME_CHANGED) {
      // Make a priority change
      reorderJobs(job, oldInfo);
    } else if (statusEvent.getEventType() == EventType.RUN_STATE_CHANGED) {
      // Check if the job is complete
      int runState = statusEvent.getNewStatus().getRunState();
      if (runState == JobStatus.SUCCEEDED
          || runState == JobStatus.FAILED
          || runState == JobStatus.KILLED) {
        jobCompleted(oldInfo);
      }
    }
  }
}
</code></pre><p>因为此时Job并未结束。从此可以看出，Job在初始化完毕后，线程池又去执行其他Job的初始化等操作，等待TaskTracker来取。<br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/10/12/Job的MapReduce Task初始化/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Job提交的过程" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/10/10/Job提交的过程/" class="article-date">
  	<time datetime="2015-10-10T04:34:18.000Z" itemprop="datePublished">2015-10-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/10/Job提交的过程/">Job提交的过程</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Job_u63D0_u4EA4_u7684_u8FC7_u7A0B"><a href="#Job_u63D0_u4EA4_u7684_u8FC7_u7A0B" class="headerlink" title="Job提交的过程"></a>Job提交的过程</h2><p>【转】<a href="http://www.cnblogs.com/esingchan/p/3917213.html" target="_blank" rel="external">原文</a><br>在分析之前，我们先进行一下粗略的思考，如果要我们自己设计分布式计算，应该怎么设计呢？假定有100个任务要并发执行，每个任务分别针对一块数据，这些数据本身是分布在多个机器上的，主要面临哪些问题？</p>
<p>1、数据如何分布是首先面临的问题，可能也是影响分布式计算性能的最关键问题。一个超大文件，按照哪种方式切割开来，分别丢到不同的机器？Hadoop的答案是按照64MB或者128MB等长切割，如果切的太小，要记录的元数据信息太多，如果切得太大，负载均衡可能比较差。如果按照某种方式切割开来后，怎么丢到多个机器？我觉得一种最简单的方案就是哪台机器剩余空间越多，就往哪台机器上丢。这样在存储空间上可以保持差不多。而且，不太会存在热点问题。这个以后分析HDFS的时候再看；</p>
<p>2、假如已经分布好了，那么任务的计算就在数据所在机器执行。哪个机器拥有什么数据，哪个机器就负责对自己拥有那块数据进行处理。任务的划分就取决于数据的划分，比如一个机器拥有了5个64MB，那就给他分配5个任务，各自处理一块，这种符合直觉的思维也是Hadoop采用的。当然，实际上没这么简单，比如有的机器可能数据分布就是多了些，那么岂不是这台机器会成为短板，整个作业的执行时间依赖于最慢这台机器执行的时间。在Hadoop里，为了解决这个问题，会把一个任务可能丢到两个机器上执行，谁先处理完，另一个就停了它。不管怎样，只要涉及到任务分配，是不是至少有一台机器负责这个分配过程吧？所以MapReduce有一个主控节点。不过理论上看也未必，假如大家按照某种同样的算法就能知道什么样的任务该到哪台机器上算，整个分布式计算集群也可以是P2P的。</p>
<p>3、关键的问题来了，如果数据之间存在关联怎么办？比如一个任务的执行需要依赖于另一个任务执行完毕才能开始，这种同步等待是并行计算的大忌，有可能大部分时间会花在等待上，甚至还不如串行执行？Hadoop里面假设的就是任务可以完全并行，如果需要关联，那么，就在这些并行任务之后，再启动一个任务去关联，后面的这种任务就是Reduce，不过天底下不一定都能这么理想地分，所以我觉得Hadoop还是有一定先天缺陷的，模型还是简单了点。</p>
<p>下面开始分析。</p>
<p>MapReduce集群包含一个JobTracker和多个TaskTracker，这里先不考虑YARN，仍然依据1版本进行分析。</p>
<p>一个MapReduce作业在Hadoop中称为Job，而JobTracker顾名思义就是对Job进行管理的节点，一个Job包含多个Map和Reduce任务，在Hadoop里Map和Reduce任务称为Task，而Job指的是Map-Reduce流程的称呼。一个Job包含多个Map Task和Reduce Task，在看作业提交代码之前，需要有一些基本的认识：</p>
<p>1、Job所需要的输入数据、资源（数据分布信息、参数配置等等）都存放于HDFS之上，其中资源信息需要Job客户端先提交至HDFS之上，这些资源信息并不传输至JobTracker，因为JobTracker本身也能随便访问HDFS，所以JobTracker是去HDFS中获得相应信息后再进行Map和Reduce Task分配；</p>
<p>2、JobClient和JobTracker可以看作CS结构，JobClient往HDFS中存入资源后，会朝JobTracker提交作业，至于到底传输给JobTracker些什么内容，实际上只是一个Job ID以及Job所在的HDFS文件目录等基本信息，需要注意的是，他们之间并不直接传递任何计算数据和资源数据，因为他们都是HDFS的客户端，都可以访问HDFS系统。</p>
<p>3、JobTracker的主要任务就是分配作业，所谓分配作业，说白了，就是将一个Job分为多个Map和Reduce任务，然后指定这些任务到底由哪些机器执行，执行任务的机器即为TaskTracker，作业到底分为多少个任务，这在传统的MPI编程中是由程序员指定的，分好任务后，任务到底在哪些机器上执行，这也是需要程序员指定的；MapReduce的不同在于，这个作业切分的过程，以及任务在哪些机器上执行的问题，是由Hadoop自己搞定的，程序员需要做的就是先将要计算的数据放到HDFS上，把Map和Reduce任务执行的（1份！）代码编写好，然后启动即可，数据到底放在了哪些机器，程序员可以不关心（查看HDFS管理信息才知道），编写的代码到底在哪些机器上（被自动拷贝多份！）执行，程序员也不关心（当然，非要去查看也是可以看到的）。</p>
<p>4、JobTracker分配好任务后，并不是直接通知TaskTracker，而是等着TaskTracker自己来取，这种设计可能是考虑MapReduce作业一般执行时间较长，比如几十分钟以上；而且JobTracker的压力不宜过大，趁着心跳时一起把任务信息获取了，否则单点容易形成瓶颈。JobTracker和TaskTracker之间存在心跳机制，可以配置，比如5秒（心跳频繁又会带来集群单点瓶颈难以扩展的问题，因为大家都跟JobTracker心跳，压力山大啊），因此，在JobClient向HDFS提交资源信息，并向JobTracker提交作业后Job进入作业队列，JobTracker从队列中取出Job并分配好Map/Reduce任务后的几秒后，TaskTracker可能才知道自己应该执行任务。这一作业启动过程时间一般都要几秒，延时较大，无法支持实时处理，这一点经常被Spark拿来鄙视，但Spark集群规模扩展后难道不存在单点瓶颈？但凡是单点分配任务的集群，不可避免都会遇到这个问题。除非分配任务的节点也可以扩展。</p>
<p>5、Map Task和Reduce Task最终运行于TaskTracker之上，TaskTracker一般是一台安装了JAVA虚拟机的Linux服务器，启动Map Task和Reduce Task时会启动JAVA虚拟机，执行Map或Reduce任务（因此Map、Reduce都是一个个JAVA进程），JAVA虚拟机启动的速度本身还是比较快，运行完毕后通知JobTracker，关闭JAVA虚拟机。一个TaskTracker可以启动很多个JAVA进程执行很多Map和Reduce任务，在YARN（Hadoop 2.0的分布式资源管理系统）中，可以指定一个Map、Reduce任务需要多少CPU核和内存，目前PC服务器一般有几十个核，和64GB以上内存，所以执行几十个Map/Reduce任务也是正常的；在YARN之前，可以配置一台服务器可以执行多少个Map/Reduce任务，但并不考虑各个Map/Reduce任务消耗资源的区别。</p>
<p>6、JobClient利用RPC机制请求JobTracker的服务，比如分配Job ID、启动作业、停止作业、查看Job进展等等。RPC机制是Hadoop里面一个很核心的部分，理解RPC机制是理解Hadoop的前提。JobTracker是MapReduce中最重要的一个类，实现了很多接口：</p>
<pre><code>public class JobTracker implements MRConstants, InterTrackerProtocol,
JobSubmissionProtocol, TaskTrackerManager, RefreshUserMappingsProtocol,
RefreshAuthorizationPolicyProtocol, AdminOperationsProtocol,
JobTrackerMXBean {
......
</code></pre><p>其中，JobSubmissionProtocol就是JobClient和JobTracker之间RPC的服务接口。这个接口的实现类就是JobTracker，包含的功能主要有：</p>
<pre><code>interface JobSubmissionProtocol extends VersionedProtocol {
public JobID getNewJobId() throws IOException;
public JobStatus submitJob(JobID jobName, String jobSubmitDir, Credentials ts) 
throws IOException;
public ClusterStatus getClusterStatus(boolean detailed) throws IOException;
public void killJob(JobID jobid) throws IOException;
public void setJobPriority(JobID jobid, String priority) 
                                                 throws IOException;
public boolean killTask(TaskAttemptID taskId, boolean shouldFail) throws IOException; 
public JobProfile getJobProfile(JobID jobid) throws IOException;
public JobStatus getJobStatus(JobID jobid) throws IOException;
public Counters getJobCounters(JobID jobid) throws IOException;   
public TaskReport[] getMapTaskReports(JobID jobid) throws IOException;
public TaskReport[] getReduceTaskReports(JobID jobid) throws IOException;
public TaskReport[] getCleanupTaskReports(JobID jobid) throws IOException;
public TaskReport[] getSetupTaskReports(JobID jobid) throws IOException;
........
}
</code></pre><p>在JobClient这一端，使用动态代理机制（至于什么是动态代理，参考JAVA Proxy、InvocationHandler相关类），在调用JobSubmissionProtocol的下面方法（这个方法在Job客户端并没有具体实现）时：</p>
<pre><code>public JobID getNewJobId() throws IOException;
</code></pre><p>进入代理类相关方法（invoke），以RPC机制往JobTracker发送相应请求（方法+参数），JobTracker接收到请求后，处理后返回结果。</p>
<p>下面进入代码分析。上一节结尾时分析到Job的作业提交方法：</p>
<pre><code>  /**
* Submit the job to the cluster and return immediately.
* @throws IOException
*/
public void submit() throws IOException, InterruptedException,
                          ClassNotFoundException {
ensureState(JobState.DEFINE);
setUseNewAPI();
// Connect to the JobTracker and submit the job
connect();
info = jobClient.submitJobInternal(conf);
super.setJobID(info.getID());
state = JobState.RUNNING;
}
</code></pre><p>先来看connect()方法，掌握RPC机制，再分析submitJobInternal的作业提交过程：</p>
<hr>
<p>0、JobClient与JobTracker之间的RPC机制</p>
<pre><code>private void connect() throws IOException, InterruptedException {
ugi.doAs(new PrivilegedExceptionAction&lt;Object&gt;() {
  public Object run() throws IOException {
    jobClient = new JobClient((JobConf) getConfiguration());    
    return null;
  }
});
}
</code></pre><p>在该方法中创建了一个JobClient对象，在其构造函数中调用init方法创建一个代理类：</p>
<pre><code>public JobClient(JobConf conf) throws IOException {
setConf(conf);
init(conf);
}
public void init(JobConf conf) throws IOException {
String tracker = conf.get(&quot;mapred.job.tracker&quot;, &quot;local&quot;);
tasklogtimeout = conf.getInt(
  TASKLOG_PULL_TIMEOUT_KEY, DEFAULT_TASKLOG_TIMEOUT);
this.ugi = UserGroupInformation.getCurrentUser();
if (&quot;local&quot;.equals(tracker)) {
  conf.setNumMapTasks(1);
  this.jobSubmitClient = new LocalJobRunner(conf);
} else {
  this.rpcJobSubmitClient = 
      createRPCProxy(JobTracker.getAddress(conf), conf);
  this.jobSubmitClient = createProxy(this.rpcJobSubmitClient, conf);
}        
}
</code></pre><p>rpcJobSubmitClient 是一个JobSubmissionProtocol 对象，而JobSubmissionProtocol 是一个与JobTracker服务相关的RPC接口，提供了一些服务访问方法。</p>
<pre><code>private JobSubmissionProtocol rpcJobSubmitClient;
</code></pre><p>在createRPCProxy方法中，调用了Hadoop RPC的创建代理的方法RPC.getProxy：</p>
<pre><code> private static JobSubmissionProtocol createRPCProxy(InetSocketAddress addr,
  Configuration conf) throws IOException {
JobSubmissionProtocol rpcJobSubmitClient = 
    (JobSubmissionProtocol)RPC.getProxy(
        JobSubmissionProtocol.class,
        JobSubmissionProtocol.versionID, addr, 
        UserGroupInformation.getCurrentUser(), conf,
        NetUtils.getSocketFactory(conf, JobSubmissionProtocol.class), 
        0,
        RetryUtils.getMultipleLinearRandomRetry(
            conf,
            MAPREDUCE_CLIENT_RETRY_POLICY_ENABLED_KEY,
            MAPREDUCE_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,
            MAPREDUCE_CLIENT_RETRY_POLICY_SPEC_KEY,
            MAPREDUCE_CLIENT_RETRY_POLICY_SPEC_DEFAULT
            ),
        false);
return rpcJobSubmitClient;
}
</code></pre><p>这个方法里面核心的是创建一个Invoker对象：</p>
<pre><code>/** Construct a client-side proxy object that implements the named protocol,
* talking to a server at the named address. */
public static VersionedProtocol getProxy(
  Class&lt;? extends VersionedProtocol&gt; protocol,
  long clientVersion, InetSocketAddress addr, UserGroupInformation ticket,
  Configuration conf, SocketFactory factory, int rpcTimeout,
  RetryPolicy connectionRetryPolicy,
  boolean checkVersion) throws IOException {
if (UserGroupInformation.isSecurityEnabled()) {
  SaslRpcServer.init(conf);
}
final Invoker invoker = new Invoker(protocol, addr, ticket, conf, factory,
    rpcTimeout, connectionRetryPolicy);
VersionedProtocol proxy = (VersionedProtocol)Proxy.newProxyInstance(
    protocol.getClassLoader(), new Class[]{protocol}, invoker);
if (checkVersion) {
  checkVersion(protocol, clientVersion, proxy);
}
return proxy;
}
</code></pre><p>而Invoker是一个实现了java.lang.reflect.InvocationHandler的类，负责代理JobSubmissionProtocol的各种服务，当JobClient调用JobSubmissionProtocol的方法（比如JobID getNewJobId() ）时，会进入Invoker的invoke方法，而在该方法中，会将调用的方法信息打包送至JobTracker执行：</p>
<pre><code>private static class Invoker implements InvocationHandler {
private Client.ConnectionId remoteId;
private Client client;
private boolean isClosed = false;
.......
public Object invoke(Object proxy, Method method, Object[] args)
  throws Throwable {
......
  ObjectWritable value = (ObjectWritable)
    client.call(new Invocation(method, args), remoteId);
.....return value.get();
}
</code></pre><p>client是RPC客户端，其call方法会创建一个Invocation对象，该对象封装了要调用的方法信息：</p>
<pre><code>private static class Invocation implements Writable, Configurable {
private String methodName;
private Class[] parameterClasses;
private Object[] parameters;
private Configuration conf;
.......
</code></pre><p>然后在call方法中，将这一信息序列化（Invocation 是一个可序列化对象）后送出去：</p>
<pre><code>/** Make a call, passing &lt;code&gt;param&lt;/code&gt;, to the IPC server defined by
* &lt;code&gt;remoteId&lt;/code&gt;, returning the value.  
* Throws exceptions if there are network problems or if the remote code 
* threw an exception. */
public Writable call(Writable param, ConnectionId remoteId)  
                   throws InterruptedException, IOException {
Call call = new Call(param);
Connection connection = getConnection(remoteId, call);
connection.sendParam(call);                 // send the parameter
boolean interrupted = false;
synchronized (call) {
  while (!call.done) {
    try {
      call.wait();                           // wait for the result
    } catch (InterruptedException ie) {
      // save the fact that we were interrupted
      interrupted = true;
    }
  }
......
}
}
</code></pre><p>sendParam(call)这一方法就是发送出去的代码：</p>
<pre><code>/** Initiates a call by sending the parameter to the remote server.
 * Note: this is not called from the Connection thread, but by other
 * threads.
 */
public void sendParam(Call call) {
  if (shouldCloseConnection.get()) {
    return;
  }
  DataOutputBuffer d=null;
  try {
    synchronized (this.out) {
      if (LOG.isDebugEnabled())
        LOG.debug(getName() + &quot; sending #&quot; + call.id);

      //for serializing the
      //data to be written
      d = new DataOutputBuffer();
      d.writeInt(call.id);
      call.param.write(d);
      byte[] data = d.getData();
      int dataLength = d.getLength();
      out.writeInt(dataLength);      //first put the data length
      out.write(data, 0, dataLength);//write the data
      out.flush();
    }
  } catch(IOException e) {
    markClosed(e);
  } finally {
    //the buffer is just an in-memory buffer, but it is still polite to
    // close early
    IOUtils.closeStream(d);
  }
}  
</code></pre><p>理解了上面的RPC机制，再来看作业提交函数submitJobInternal的执行。<br>submitJobInternal函数是JobClient向JobTracker提交作业的核心方法:</p>
<pre><code>  /**
* Internal method for submitting jobs to the system.
* @param job the configuration to submit
* @return a proxy object for the running job
* @throws FileNotFoundException
* @throws ClassNotFoundException
* @throws InterruptedException
* @throws IOException
*/
public RunningJob submitJobInternal(final JobConf job
                           ) throws FileNotFoundException, 
                                    ClassNotFoundException,
                                    InterruptedException,
                                    IOException {
/*
* configure the command line options correctly on the submitting dfs
*/
return ugi.doAs(new PrivilegedExceptionAction&lt;RunningJob&gt;() {
  public RunningJob run() throws FileNotFoundException, 
  ClassNotFoundException,
  InterruptedException,
  IOException{
    JobConf jobCopy = job;
    Path jobStagingArea = JobSubmissionFiles.getStagingDir(JobClient.this,
        jobCopy);
    JobID jobId = jobSubmitClient.getNewJobId();
    Path submitJobDir = new Path(jobStagingArea, jobId.toString());
    jobCopy.set(&quot;mapreduce.job.dir&quot;, submitJobDir.toString());
    JobStatus status = null;
    try {
      populateTokenCache(jobCopy, jobCopy.getCredentials());
      copyAndConfigureFiles(jobCopy, submitJobDir);
      // get delegation token for the dir
      TokenCache.obtainTokensForNamenodes(jobCopy.getCredentials(),
                                          new Path [] {submitJobDir},
                                          jobCopy);
      Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);
      int reduces = jobCopy.getNumReduceTasks();
      InetAddress ip = InetAddress.getLocalHost();
      if (ip != null) {
        job.setJobSubmitHostAddress(ip.getHostAddress());
        job.setJobSubmitHostName(ip.getHostName());
      }
      JobContext context = new JobContext(jobCopy, jobId);
      // Check the output specification
      if (reduces == 0 ? jobCopy.getUseNewMapper() : 
        jobCopy.getUseNewReducer()) {
        org.apache.hadoop.mapreduce.OutputFormat&lt;?,?&gt; output =
          ReflectionUtils.newInstance(context.getOutputFormatClass(),
              jobCopy);
        output.checkOutputSpecs(context);
      } else {
        jobCopy.getOutputFormat().checkOutputSpecs(fs, jobCopy);
      }

      jobCopy = (JobConf)context.getConfiguration();
      // Create the splits for the job
      FileSystem fs = submitJobDir.getFileSystem(jobCopy);
      LOG.debug(&quot;Creating splits at &quot; + fs.makeQualified(submitJobDir));
      int maps = writeSplits(context, submitJobDir);
      jobCopy.setNumMapTasks(maps);
      // write &quot;queue admins of the queue to which job is being submitted&quot;
      // to job file.
      String queue = jobCopy.getQueueName();
      AccessControlList acl = jobSubmitClient.getQueueAdmins(queue);
      jobCopy.set(QueueManager.toFullPropertyName(queue,
          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getACLString());
      // Write job file to JobTracker&apos;s fs        
      FSDataOutputStream out = 
        FileSystem.create(fs, submitJobFile,
            new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));
      // removing jobtoken referrals before copying the jobconf to HDFS
      // as the tasks don&apos;t need this setting, actually they may break
      // because of it if present as the referral will point to a
      // different job.
      TokenCache.cleanUpTokenReferral(jobCopy);
      try {
        jobCopy.writeXml(out);
      } finally {
        out.close();
      }
      //
      // Now, actually submit the job (using the submit name)
      //
      printTokens(jobId, jobCopy.getCredentials());
      status = jobSubmitClient.submitJob(
          jobId, submitJobDir.toString(), jobCopy.getCredentials());
      JobProfile prof = jobSubmitClient.getJobProfile(jobId);
      if (status != null &amp;&amp; prof != null) {
        return new NetworkedJob(status, prof, jobSubmitClient);
      } else {
        throw new IOException(&quot;Could not launch job&quot;);
      }
    } finally {
      if (status == null) {
        LOG.info(&quot;Cleaning up the staging area &quot; + submitJobDir);
        if (fs != null &amp;&amp; submitJobDir != null)
          fs.delete(submitJobDir, true);
      }
    }
  }
});
}
</code></pre><hr>
<p>该方法比较复杂，主要分为几个步骤，我们对其分解后逐一分析：<br>1、获取Job所在HDFS中的根目录</p>
<pre><code>Path jobStagingArea = JobSubmissionFiles.getStagingDir(JobClient.this,jobCopy); 
</code></pre><p>在MapReduce中，所有Job的信息（不是输入数据）都会存放于某个根目录下，这个根目录称为staging目录，Staging在英文中含义是临时工作台、脚手架等，个人理解他的意思是要执行MapReduce作业的这些框架信息（比如数据分布信息，Job配置参数等等）存放在这里。由参数mapreduce.jobtracker.staging.root.dir配置，默认是“/tmp/hadoop/mapred/staging”<br>这可以由getStagingDir方法的内部看到，JobClient里的这个方法最终会调用JobSubmissionFiles的getStagingDir方法：</p>
<pre><code>public static Path getStagingDir(JobClient client, Configuration conf)  throws IOException, InterruptedException {
Path stagingArea = client.getStagingAreaDir();
FileSystem fs = stagingArea.getFileSystem(conf);
。。。。
return stagingArea;
}
</code></pre><p>可见，调用了client的getStagingAreaDir方法：</p>
<pre><code>public Path getStagingAreaDir() throws IOException {
if (stagingAreaDir == null) {
  stagingAreaDir = new Path(jobSubmitClient.getStagingAreaDir());
}
return stagingAreaDir;
} 
</code></pre><p>最终，调用jobSubmitClient的getStagingAreaDir方法，jobSubmitClient是一个JobSubmissionProtocol接口对象，通过动态代理，调用了位于服务端JobTracker的同名方法：</p>
<pre><code>public String getStagingAreaDir() throws IOException {
// Check for safe-mode
checkSafeMode();
try{
  final String user =
    UserGroupInformation.getCurrentUser().getShortUserName();
  return getMROwner().doAs(new PrivilegedExceptionAction&lt;String&gt;() {
    @Override
    public String run() throws Exception {
      return getStagingAreaDirInternal(user);
    }
  });
} catch(InterruptedException ie) {
  throw new IOException(ie);
}
}
</code></pre><p>其中调用了getStagingAreaDirInternal方法：</p>
<pre><code>private String getStagingAreaDirInternal(String user) throws IOException {
final Path stagingRootDir =
  new Path(conf.get(&quot;mapreduce.jobtracker.staging.root.dir&quot;,
        &quot;/tmp/hadoop/mapred/staging&quot;));
final FileSystem fs = stagingRootDir.getFileSystem(conf);
return fs.makeQualified(new Path(stagingRootDir,
                          user+&quot;/.staging&quot;)).toString();
}
</code></pre><p>得到该目录后，通过RPC返回。user是用户名，比如esingchan，则目录为：/tmp/hadoop/mapred/staging/esingchan/.staging/</p>
<hr>
<p>2、获取Job ID和Job工作目录（但还没创建）：</p>
<pre><code>JobID jobId = jobSubmitClient.getNewJobId();
Path submitJobDir = new Path(jobStagingArea, jobId.toString());
jobCopy.set(&quot;mapreduce.job.dir&quot;, submitJobDir.toString());
</code></pre><p>getNewJobId这个方法也是通过RPC获得ID信息（job id从1开始递增），然后在HDFS中的staging目录下创建工作目录，将这个目录设置成mapreduce.job.dir的值，最终目录一般呈现类似这种形式：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/<br>Job ID的生成一般由时间等组成，具体在JobTracker方法getNewJobId中：</p>
<pre><code>public synchronized JobID getNewJobId() throws IOException {
// Check for JobTracker operational state
checkJobTrackerState();
return new JobID(getTrackerIdentifier(), nextJobId++);
}
</code></pre><hr>
<p>3、创建工作目录，向HDFS提交资源数据：</p>
<pre><code>copyAndConfigureFiles(jobCopy, submitJobDir);
</code></pre><p>copyAndConfigureFiles这个方法是JobClient向HDFS提交资源数据的主要方法。其实现为：</p>
<pre><code>private void copyAndConfigureFiles(JobConf job, Path jobSubmitDir) throws IOException, InterruptedException {
short replication = (short)job.getInt(&quot;mapred.submit.replication&quot;, 10);
copyAndConfigureFiles(job, jobSubmitDir, replication);
// Set the working directory
if (job.getWorkingDirectory() == null) {
  job.setWorkingDirectory(fs.getWorkingDirectory());        
}
}
private void copyAndConfigureFiles(JobConf job, Path submitJobDir, 
  short replication) throws IOException, InterruptedException {
if (!(job.getBoolean(&quot;mapred.used.genericoptionsparser&quot;, false))) {
  LOG.warn(&quot;Use GenericOptionsParser for parsing the arguments. &quot; +
           &quot;Applications should implement Tool for the same.&quot;);
}
// Retrieve command line arguments placed into the JobConf
// by GenericOptionsParser.
String files = job.get(&quot;tmpfiles&quot;);
String libjars = job.get(&quot;tmpjars&quot;);
String archives = job.get(&quot;tmparchives&quot;);
//
// Figure out what fs the JobTracker is using.  Copy the
// job to it, under a temporary name.  This allows DFS to work,
// and under the local fs also provides UNIX-like object loading 
// semantics.  (that is, if the job file is deleted right after
// submission, we can still run the submission to completion)
//
// Create a number of filenames in the JobTracker&apos;s fs namespace
FileSystem fs = submitJobDir.getFileSystem(job);
LOG.debug(&quot;default FileSystem: &quot; + fs.getUri());
if (fs.exists(submitJobDir)) {
  throw new IOException(&quot;Not submitting job. Job directory &quot; + submitJobDir
      +&quot; already exists!! This is unexpected.Please check what&apos;s there in&quot; +
      &quot; that directory&quot;);
}
submitJobDir = fs.makeQualified(submitJobDir);
FsPermission mapredSysPerms = new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);
FileSystem.mkdirs(fs, submitJobDir, mapredSysPerms);
Path filesDir = JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);
Path archivesDir = JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);
Path libjarsDir = JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);
// add all the command line files/ jars and archive
// first copy them to jobtrackers filesystem 
if (files != null) {
  FileSystem.mkdirs(fs, filesDir, mapredSysPerms);
  String[] fileArr = files.split(&quot;,&quot;);
  for (String tmpFile: fileArr) {
    URI tmpURI;
    try {
      tmpURI = new URI(tmpFile);
    } catch (URISyntaxException e) {
      throw new IllegalArgumentException(e);
    }
    Path tmp = new Path(tmpURI);
    Path newPath = copyRemoteFiles(fs,filesDir, tmp, job, replication);
    try {
      URI pathURI = getPathURI(newPath, tmpURI.getFragment());
      DistributedCache.addCacheFile(pathURI, job);
    } catch(URISyntaxException ue) {
      //should not throw a uri exception 
      throw new IOException(&quot;Failed to create uri for &quot; + tmpFile, ue);
    }
    DistributedCache.createSymlink(job);
  }
}
if (libjars != null) {
  FileSystem.mkdirs(fs, libjarsDir, mapredSysPerms);
  String[] libjarsArr = libjars.split(&quot;,&quot;);
  for (String tmpjars: libjarsArr) {
    Path tmp = new Path(tmpjars);
    Path newPath = copyRemoteFiles(fs, libjarsDir, tmp, job, replication);
    DistributedCache.addArchiveToClassPath
      (new Path(newPath.toUri().getPath()), job, fs);
  }
}
if (archives != null) {
 FileSystem.mkdirs(fs, archivesDir, mapredSysPerms); 
 String[] archivesArr = archives.split(&quot;,&quot;);
 for (String tmpArchives: archivesArr) {
   URI tmpURI;
   try {
     tmpURI = new URI(tmpArchives);
   } catch (URISyntaxException e) {
     throw new IllegalArgumentException(e);
   }
   Path tmp = new Path(tmpURI);
   Path newPath = copyRemoteFiles(fs, archivesDir, tmp, job, replication);
   try {
     URI pathURI = getPathURI(newPath, tmpURI.getFragment());
     DistributedCache.addCacheArchive(pathURI, job);
   } catch(URISyntaxException ue) {
     //should not throw an uri excpetion
     throw new IOException(&quot;Failed to create uri for &quot; + tmpArchives, ue);
   }
   DistributedCache.createSymlink(job);
 }
}
// First we check whether the cached archives and files are legal.
TrackerDistributedCacheManager.validate(job);
//  set the timestamps of the archives and files and set the
//  public/private visibility of the archives and files
TrackerDistributedCacheManager.determineTimestampsAndCacheVisibilities(job);
// get DelegationTokens for cache files
TrackerDistributedCacheManager.getDelegationTokens(job, 
                                                   job.getCredentials());
String originalJarPath = job.getJar();
if (originalJarPath != null) {           // copy jar to JobTracker&apos;s fs
  // use jar name if job is not named. 
  if (&quot;&quot;.equals(job.getJobName())){
    job.setJobName(new Path(originalJarPath).getName());
  }
  Path originalJarFile = new Path(originalJarPath);
  URI jobJarURI = originalJarFile.toUri();
  // If the job jar is already in fs, we don&apos;t need to copy it from local fs
  if (jobJarURI.getScheme() == null || jobJarURI.getAuthority() == null
          || !(jobJarURI.getScheme().equals(fs.getUri().getScheme())
              &amp;&amp; jobJarURI.getAuthority().equals(
                                        fs.getUri().getAuthority()))) {
    Path submitJarFile = JobSubmissionFiles.getJobJar(submitJobDir);
    job.setJar(submitJarFile.toString());
    fs.copyFromLocalFile(originalJarFile, submitJarFile);
    fs.setReplication(submitJarFile, replication);
    fs.setPermission(submitJarFile, 
        new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));
  }
} else {
  LOG.warn(&quot;No job jar file set.  User classes may not be found. &quot;+
           &quot;See JobConf(Class) or JobConf#setJar(String).&quot;);
}
}
</code></pre><p>其中:</p>
<pre><code>short replication = (short)job.getInt(&quot;mapred.submit.replication&quot;, 10);
</code></pre><p>表示这部分资源数据可靠性要求很高，在HDFS中默认保留10份。</p>
<p>copyAndConfigureFiles中以下几行代码：</p>
<pre><code>String files = job.get(&quot;tmpfiles&quot;);
String libjars = job.get(&quot;tmpjars&quot;);
String archives = job.get(&quot;tmparchives&quot;);
Path filesDir = JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);
Path archivesDir = JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);
Path libjarsDir = JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);
FileSystem.mkdirs(fs, filesDir, mapredSysPerms);
Path newPath = copyRemoteFiles(fs,filesDir, tmp, job, replication);
FileSystem.mkdirs(fs, archivesDir, mapredSysPerms); 
Path newPath = copyRemoteFiles(fs, archivesDir, tmp, job, replication);
FileSystem.mkdirs(fs, libjarsDir, mapredSysPerms);
Path newPath = copyRemoteFiles(fs, libjarsDir, tmp, job, replication);
</code></pre><p>分别在submitJorDir的目录下创建临时文件、Jar包文件、归档文件的目录，根据JobSubmissionFiles可以看出其目录名：</p>
<pre><code>public static Path getJobDistCacheFiles(Path jobSubmitDir) {
return new Path(jobSubmitDir, &quot;files&quot;);
}
/**
* Get the job distributed cache archives path.
* @param jobSubmitDir 
*/
public static Path getJobDistCacheArchives(Path jobSubmitDir) {
return new Path(jobSubmitDir, &quot;archives&quot;);
}
/**
* Get the job distributed cache libjars path.
* @param jobSubmitDir 
*/
public static Path getJobDistCacheLibjars(Path jobSubmitDir) {
return new Path(jobSubmitDir, &quot;libjars&quot;);
}
</code></pre><p>可见，创建后的目录示例：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/files<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/archives<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/libjars<br>之后，将用户程序的Jar包从本地文件系统上传到HDFS中，见下面的copyFromLocalFile方法：</p>
<pre><code>String originalJarPath = job.getJar();
if (originalJarPath != null) {           // copy jar to JobTracker&apos;s fs
  // use jar name if job is not named. 
  if (&quot;&quot;.equals(job.getJobName())){
    job.setJobName(new Path(originalJarPath).getName());
  }
  Path originalJarFile = new Path(originalJarPath);
  URI jobJarURI = originalJarFile.toUri();
  // If the job jar is already in fs, we don&apos;t need to copy it from local fs
  if (jobJarURI.getScheme() == null || jobJarURI.getAuthority() == null
          || !(jobJarURI.getScheme().equals(fs.getUri().getScheme())
              &amp;&amp; jobJarURI.getAuthority().equals(
                                        fs.getUri().getAuthority()))) {
    Path submitJarFile = JobSubmissionFiles.getJobJar(submitJobDir);
    job.setJar(submitJarFile.toString());
    fs.copyFromLocalFile(originalJarFile, submitJarFile);
    fs.setReplication(submitJarFile, replication);
    fs.setPermission(submitJarFile, 
        new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));
</code></pre><p>而jar包所在目录即Job根目录，其文件名为：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/files/job.jar，其名字由下面的方法指定：</p>
<pre><code>public static Path getJobJar(Path jobSubmitDir) {
return new Path(jobSubmitDir, &quot;job.jar&quot;);
}
</code></pre><p>在HDFS中创建好该文件后，将JobClient本地编译好的jar文件复制至该文件。注意，libjars里面的文件是需要的其它临时jar库文件。<br>上面创建好的目录里存放的内容分别为：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/libjars<br>用于存储执行job.jar需要的其它jar库文件。<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/archives<br>用于存储任务需要归档的一些文件（执行耗时完成时间啊等等各种归档信息）<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/files<br>用于存储执行程序所需的一些输入文件，注意不是计算的数据，而是其它一些文件。<br>到这一步后，实际上还没有完毕，还有两个文件需要上传，job.split和job.xml，分别代表job的文件分割信息和运行参数配置信息，但这两部分需要客户端解析分析才能得到，所以放在后面介绍。</p>
<hr>
<p>4、创建Job配置文件<br>上面的目录为作业执行提供了数据基础，但关于任务数量等等任务配置信息还没有创建。任务配置信息不完全是用户直接指定的，而是需要进行一些分析获得。所有配置信息最终会由客户端JobClient生成后创建于HDFS的Staging目录下，即job.xml文件，主要包含任务参数，记录如Reduce数量等：</p>
<pre><code>Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);
public static Path getJobConfPath(Path jobSubmitDir) {
return new Path(jobSubmitDir, &quot;job.xml&quot;);
}
</code></pre><p>其过程中从用户指定的配置文件（其实也是xml文件，但是位于本地Linux文件系统）中获取一些参数，分析得到作业所需参数。主要获取的参数有：<br>获取Reduce数量：</p>
<pre><code>int reduces = jobCopy.getNumReduceTasks();
</code></pre><p>获取客户端IP信息：</p>
<pre><code>InetAddress ip = InetAddress.getLocalHost();
if (ip != null) {
job.setJobSubmitHostAddress(ip.getHostAddress());
job.setJobSubmitHostName(ip.getHostName());
}
</code></pre><p>获取Map任务数量，这一信息是分析得到的，并不是直接获取到，获得的分片信息会以文件名job.split上传至HDFS：<br>创建Job参数，这些参数太多，使用JobContext类表示：</p>
<pre><code>JobContext context = new JobContext(jobCopy, jobId);
</code></pre><p>注意，一般而言，Context中文含义是上下文，在Hadoop里一般用于记录参数，这里就是记录Job相关的参数，比如Map的实现类名等等，其声明为：</p>
<pre><code>public class JobContext {
// Put all of the attribute names in here so that Job and JobContext are
// consistent.
protected static final String INPUT_FORMAT_CLASS_ATTR = 
&quot;mapreduce.inputformat.class&quot;;
protected static final String MAP_CLASS_ATTR = &quot;mapreduce.map.class&quot;;
protected static final String COMBINE_CLASS_ATTR = &quot;mapreduce.combine.class&quot;;
protected static final String REDUCE_CLASS_ATTR = &quot;mapreduce.reduce.class&quot;;
protected static final String OUTPUT_FORMAT_CLASS_ATTR = 
&quot;mapreduce.outputformat.class&quot;;
protected static final String PARTITIONER_CLASS_ATTR = 
&quot;mapreduce.partitioner.class&quot;;
protected final org.apache.hadoop.mapred.JobConf conf;
protected final Credentials credentials;
private JobID jobId;
public static final String JOB_NAMENODES = &quot;mapreduce.job.hdfs-servers&quot;;
public static final String JOB_ACL_VIEW_JOB = &quot;mapreduce.job.acl-view-job&quot;;
public static final String JOB_ACL_MODIFY_JOB =
&quot;mapreduce.job.acl-modify-job&quot;;
public static final String CACHE_FILE_VISIBILITIES = 
&quot;mapreduce.job.cache.files.visibilities&quot;;
public static final String CACHE_ARCHIVES_VISIBILITIES = 
&quot;mapreduce.job.cache.archives.visibilities&quot;;
public static final String JOB_CANCEL_DELEGATION_TOKEN = 
&quot;mapreduce.job.complete.cancel.delegation.tokens&quot;;
public static final String USER_LOG_RETAIN_HOURS = 
&quot;mapred.userlog.retain.hours&quot;;
/**
* The UserGroupInformation object that has a reference to the current user
*/
protected UserGroupInformation ugi;
。。。。。。
</code></pre><p>实际上，JobContext就是封装了JobConf的一个类，JobConf相当于客户端最初本地的配置文件信息，被解析后封装为JobConf对象，而JobContext则可以查询JobConf得到相应信息，比如返回客户端的Mapper实现类：</p>
<pre><code>public Class&lt;? extends Mapper&lt;?,?,?,?&gt;&gt; getMapperClass() 
 throws ClassNotFoundException {
return (Class&lt;? extends Mapper&lt;?,?,?,?&gt;&gt;) 
  conf.getClass(MAP_CLASS_ATTR, Mapper.class);
}
</code></pre><p>创建了JobContext后，用于检查输出目录的有效性：</p>
<pre><code>// Check the output specification
if (reduces == 0 ? jobCopy.getUseNewMapper() : 
  jobCopy.getUseNewReducer()) {
  org.apache.hadoop.mapreduce.OutputFormat&lt;?,?&gt; output =
    ReflectionUtils.newInstance(context.getOutputFormatClass(),
        jobCopy);
  output.checkOutputSpecs(context);
} else {
  jobCopy.getOutputFormat().checkOutputSpecs(fs, jobCopy);
}
</code></pre><p>之后，分析输入数据的Split分割信息，用于产生Map的数量：</p>
<pre><code>// Create the splits for the job
FileSystem fs = submitJobDir.getFileSystem(jobCopy);
LOG.debug(&quot;Creating splits at &quot; + fs.makeQualified(submitJobDir));
int maps = writeSplits(context, submitJobDir);
jobCopy.setNumMapTasks(maps);
</code></pre><p>注意，对输入数据进行Split，也就决定了Map的数量，可以说，Map的数量是JobClient根据SplitSize产生的，Reduce的数量是用户指定的，但Map和Reduce具体运行在哪些机器由JobTracker分配。<br>在上面的代码中，获得的分片信息会以文件名job.split上传至HDFS：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/files/job.split<br>这一部分的逻辑也有些复杂，我们暂时认为已经产生了job.split文件，在第6节进行详细分析。<br>在得到上述信息后，准备将信息写入Job配置文件Job.xml：</p>
<pre><code>// Write job file to JobTracker&apos;s fs        
FSDataOutputStream out = 
  FileSystem.create(fs, submitJobFile,
      new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));
// removing jobtoken referrals before copying the jobconf to HDFS
// as the tasks don&apos;t need this setting, actually they may break
// because of it if present as the referral will point to a
// different job.
TokenCache.cleanUpTokenReferral(jobCopy);
try {
  jobCopy.writeXml(out);
} finally {
  out.close();
}
</code></pre><p>submitJobFile即：<br>/tmp/hadoop/mapred/staging/esingchan/.staging/job_201408161410_0001/files/job.xml</p>
<hr>
<p>5、向JobTracker真正提交Job<br>真正提交Job的代码为：</p>
<pre><code>status = jobSubmitClient.submitJob(
    jobId, submitJobDir.toString(), jobCopy.getCredentials());
JobProfile prof = jobSubmitClient.getJobProfile(jobId);
</code></pre><p>jobSubmitClient调用的这个方法同样利用RPC传递到JobTracker执行同名方法，可以看到，JobClient和JobTracker两者传递的内容实际上主要有两个，一个是Job ID，另一个是该Job在HDFS中的根目录，具体的资源数据等等实际上全部由JobClient预先放在HDFS中。其代码为：</p>
<pre><code>JobStatus submitJob(JobID jobId, String jobSubmitDir,
  UserGroupInformation ugi, Credentials ts, boolean recovered)
  throws IOException {
// Check for safe-mode
checkSafeMode();
JobInfo jobInfo = null;
if (ugi == null) {
  ugi = UserGroupInformation.getCurrentUser();
}
synchronized (this) {
  if (jobs.containsKey(jobId)) {
    // job already running, don&apos;t start twice
    return jobs.get(jobId).getStatus();
  }
  jobInfo = new JobInfo(jobId, new Text(ugi.getShortUserName()),
      new Path(jobSubmitDir));
}
// Store the job-info in a file so that the job can be recovered
// later (if at all)
// Note: jobDir &amp; jobInfo are owned by JT user since we are using
// his fs object
if (!recovered) {
  Path jobDir = getSystemDirectoryForJob(jobId);
  FileSystem.mkdirs(fs, jobDir, new FsPermission(SYSTEM_DIR_PERMISSION));
  FSDataOutputStream out = fs.create(getSystemFileForJob(jobId));
  jobInfo.write(out);
  out.close();
}
// Create the JobInProgress, do not lock the JobTracker since
// we are about to copy job.xml from HDFS and write jobToken file to HDFS
JobInProgress job = null;
try {
  if (ts == null) {
    ts = new Credentials();
  }
  generateAndStoreJobTokens(jobId, ts);
  job = new JobInProgress(this, this.conf, jobInfo, 0, ts);
} catch (Exception e) {
  throw new IOException(e);
}
if (recovered &amp;&amp; 
    !job.getJobConf().getBoolean(
        JobConf.MAPREDUCE_RECOVER_JOB, 
        JobConf.DEFAULT_MAPREDUCE_RECOVER_JOB)) {
  LOG.info(&quot;Job &quot;+ jobId.toString() + &quot; is not enable for recovery, cleaning up job files&quot;);
  job.cleanupJob();
  return null;
}
synchronized (this) {
  // check if queue is RUNNING
  String queue = job.getProfile().getQueueName();
  if (!queueManager.isRunning(queue)) {
    throw new IOException(&quot;Queue \&quot;&quot; + queue + &quot;\&quot; is not running&quot;);
  }
  try {
    aclsManager.checkAccess(job, ugi, Operation.SUBMIT_JOB);
  } catch (IOException ioe) {
    LOG.warn(&quot;Access denied for user &quot; + job.getJobConf().getUser()
        + &quot;. Ignoring job &quot; + jobId, ioe);
    job.fail();
    throw ioe;
  }
  // Check the job if it cannot run in the cluster because of invalid memory
  // requirements.
  try {
    checkMemoryRequirements(job);
  } catch (IOException ioe) {
    throw ioe;
  }
  try {
    this.taskScheduler.checkJobSubmission(job);
  } catch (IOException ioe){
    LOG.error(&quot;Problem in submitting job &quot; + jobId, ioe);
    throw ioe;
  }
  // Submit the job
  JobStatus status;
  try {
    status = addJob(jobId, job);
  } catch (IOException ioe) {
    LOG.info(&quot;Job &quot; + jobId + &quot; submission failed!&quot;, ioe);
    status = job.getStatus();
    status.setFailureInfo(StringUtils.stringifyException(ioe));
    failJob(job);
    throw ioe;
  }
  return status;
}
</code></pre><p>  }<br>主要分为几个步骤：<br>5.1 创建JobInProgress对象，该对象是JobTracker用来记录Job信息的类。其声明为：</p>
<pre><code>public class JobInProgress {
  JobProfile profile;
  JobStatus status;
  String jobFile = null;
  Path localJobFile = null;
  final QueueMetrics queueMetrics;
  TaskInProgress maps[] = new TaskInProgress[0];
  TaskInProgress reduces[] = new TaskInProgress[0];
  TaskInProgress cleanup[] = new TaskInProgress[0];
  TaskInProgress setup[] = new TaskInProgress[0];
  int numMapTasks = 0;
  int numReduceTasks = 0;
  final long memoryPerMap;
  final long memoryPerReduce;
  volatile int numSlotsPerMap = 1;
  volatile int numSlotsPerReduce = 1;
  final int maxTaskFailuresPerTracker;

  // Counters to track currently running/finished/failed Map/Reduce task-attempts
  int runningMapTasks = 0;
  int runningReduceTasks = 0;
  int finishedMapTasks = 0;
  int finishedReduceTasks = 0;
  int failedMapTasks = 0; 
  int failedReduceTasks = 0;
  private static long DEFAULT_REDUCE_INPUT_LIMIT = -1L;
  long reduce_input_limit = -1L;
  private static float DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART = 0.05f;
  int completedMapsForReduceSlowstart = 0;

  // runningMapTasks include speculative tasks, so we need to capture 
  // speculative tasks separately 
  int speculativeMapTasks = 0;
  int speculativeReduceTasks = 0;

  final int mapFailuresPercent;
  final int reduceFailuresPercent;
  int failedMapTIPs = 0;
  int failedReduceTIPs = 0;
  private volatile boolean launchedCleanup = false;
  private volatile boolean launchedSetup = false;
  private volatile boolean jobKilled = false;
  private volatile boolean jobFailed = false;
  JobPriority priority = JobPriority.NORMAL;
  final JobTracker jobtracker;

  protected Credentials tokenStorage;
  // NetworkTopology Node to the set of TIPs
  Map&lt;Node, List&lt;TaskInProgress&gt;&gt; nonRunningMapCache;

  // Map of NetworkTopology Node to set of running TIPs
  Map&lt;Node, Set&lt;TaskInProgress&gt;&gt; runningMapCache;
  // A list of non-local, non-running maps
  final List&lt;TaskInProgress&gt; nonLocalMaps;
  // Set of failed, non-running maps sorted by #failures
  final SortedSet&lt;TaskInProgress&gt; failedMaps;
  // A set of non-local running maps
  Set&lt;TaskInProgress&gt; nonLocalRunningMaps;
  // A list of non-running reduce TIPs
  Set&lt;TaskInProgress&gt; nonRunningReduces;
  // A set of running reduce TIPs
  Set&lt;TaskInProgress&gt; runningReduces;

  // A list of cleanup tasks for the map task attempts, to be launched
  List&lt;TaskAttemptID&gt; mapCleanupTasks = new LinkedList&lt;TaskAttemptID&gt;();

  // A list of cleanup tasks for the reduce task attempts, to be launched
  List&lt;TaskAttemptID&gt; reduceCleanupTasks = new LinkedList&lt;TaskAttemptID&gt;();
。。。。。。。
</code></pre><p>创建JobInProgress对象，对该任务进行初始化：</p>
<pre><code>JobInProgress(JobTracker jobtracker, final JobConf default_conf, 
  JobInfo jobInfo, int rCount, Credentials ts) throws IOException, InterruptedException {
try {
  this.restartCount = rCount;
  this.jobId = JobID.downgrade(jobInfo.getJobID());
  String url = &quot;http://&quot; + jobtracker.getJobTrackerMachine() + &quot;:&quot; 
  + jobtracker.getInfoPort() + &quot;/jobdetails.jsp?jobid=&quot; + jobId;
  this.jobtracker = jobtracker;
  this.status = new JobStatus(jobId, 0.0f, 0.0f, JobStatus.PREP);
  this.status.setUsername(jobInfo.getUser().toString());
  this.jobtracker.getInstrumentation().addPrepJob(conf, jobId);
  // Add the queue-level metric below (after the profile has been initialized)
  this.startTime = jobtracker.getClock().getTime();
  status.setStartTime(startTime);
  this.localFs = jobtracker.getLocalFileSystem();
  this.tokenStorage = ts;
  // use the user supplied token to add user credentials to the conf
  jobSubmitDir = jobInfo.getJobSubmitDir();
  user = jobInfo.getUser().toString();
  userUGI = UserGroupInformation.createRemoteUser(user);
  if (ts != null) {
    for (Token&lt;? extends TokenIdentifier&gt; token : ts.getAllTokens()) {
      userUGI.addToken(token);
    }
  }
  fs = userUGI.doAs(new PrivilegedExceptionAction&lt;FileSystem&gt;() {
    public FileSystem run() throws IOException {
      return jobSubmitDir.getFileSystem(default_conf);
    }});

  /** check for the size of jobconf **/
  Path submitJobFile = JobSubmissionFiles.getJobConfPath(jobSubmitDir);
  FileStatus fstatus = fs.getFileStatus(submitJobFile);
  if (fstatus.getLen() &gt; jobtracker.MAX_JOBCONF_SIZE) {
    throw new IOException(&quot;Exceeded max jobconf size: &quot; 
        + fstatus.getLen() + &quot; limit: &quot; + jobtracker.MAX_JOBCONF_SIZE);
  }
  this.localJobFile = default_conf.getLocalPath(JobTracker.SUBDIR
      +&quot;/&quot;+jobId + &quot;.xml&quot;);
  Path jobFilePath = JobSubmissionFiles.getJobConfPath(jobSubmitDir);
  jobFile = jobFilePath.toString();
  fs.copyToLocalFile(jobFilePath, localJobFile);
  conf = new JobConf(localJobFile);
  if (conf.getUser() == null) {
    this.conf.setUser(user);
  }
  if (!conf.getUser().equals(user)) {
    String desc = &quot;The username &quot; + conf.getUser() + &quot; obtained from the &quot; +
    &quot;conf doesn&apos;t match the username &quot; + user + &quot; the user &quot; +
    &quot;authenticated as&quot;;
    AuditLogger.logFailure(user, Operation.SUBMIT_JOB.name(), conf.getUser(), 
        jobId.toString(), desc);
    throw new IOException(desc);
  }

  this.priority = conf.getJobPriority();
  this.status.setJobPriority(this.priority);
  String queueName = conf.getQueueName();
  this.profile = new JobProfile(user, jobId, 
      jobFile, url, conf.getJobName(), queueName);
  Queue queue = this.jobtracker.getQueueManager().getQueue(queueName);
  if (queue == null) {
    throw new IOException(&quot;Queue \&quot;&quot; + queueName + &quot;\&quot; does not exist&quot;);
  }
  this.queueMetrics = queue.getMetrics();
  this.queueMetrics.addPrepJob(conf, jobId);
  this.submitHostName = conf.getJobSubmitHostName();
  this.submitHostAddress = conf.getJobSubmitHostAddress();
  this.numMapTasks = conf.getNumMapTasks();
  this.numReduceTasks = conf.getNumReduceTasks();
  this.memoryPerMap = conf.getMemoryForMapTask();
  this.memoryPerReduce = conf.getMemoryForReduceTask();
  this.taskCompletionEvents = new ArrayList&lt;TaskCompletionEvent&gt;
  (numMapTasks + numReduceTasks + 10);
  // Construct the jobACLs
  status.setJobACLs(jobtracker.getJobACLsManager().constructJobACLs(conf));
  this.mapFailuresPercent = conf.getMaxMapTaskFailuresPercent();
  this.reduceFailuresPercent = conf.getMaxReduceTaskFailuresPercent();
  this.maxTaskFailuresPerTracker = conf.getMaxTaskFailuresPerTracker();
  hasSpeculativeMaps = conf.getMapSpeculativeExecution();
  hasSpeculativeReduces = conf.getReduceSpeculativeExecution();
  // a limit on the input size of the reduce.
  // we check to see if the estimated input size of 
  // of each reduce is less than this value. If not
  // we fail the job. A value of -1 just means there is no
  // limit set.
  reduce_input_limit = -1L;
  this.maxLevel = jobtracker.getNumTaskCacheLevels();
  this.anyCacheLevel = this.maxLevel+1;
  this.nonLocalMaps = new LinkedList&lt;TaskInProgress&gt;();
  this.failedMaps = new TreeSet&lt;TaskInProgress&gt;(failComparator);
  this.nonLocalRunningMaps = new LinkedHashSet&lt;TaskInProgress&gt;();
  this.runningMapCache = new IdentityHashMap&lt;Node, Set&lt;TaskInProgress&gt;&gt;();
  this.nonRunningReduces = new TreeSet&lt;TaskInProgress&gt;(failComparator);
  this.runningReduces = new LinkedHashSet&lt;TaskInProgress&gt;();
  this.resourceEstimator = new ResourceEstimator(this);
  this.reduce_input_limit = conf.getLong(&quot;mapreduce.reduce.input.limit&quot;, 
      DEFAULT_REDUCE_INPUT_LIMIT);
  // register job&apos;s tokens for renewal
  DelegationTokenRenewal.registerDelegationTokensForRenewal(
      jobInfo.getJobID(), ts, jobtracker.getConf());

  // Check task limits
  checkTaskLimits();
} finally {
  //close all FileSystems that was created above for the current user
  //At this point, this constructor is called in the context of an RPC, and
  //hence the &quot;current user&quot; is actually referring to the kerberos
  //authenticated user (if security is ON).
  FileSystem.closeAllForUGI(UserGroupInformation.getCurrentUser());
}
}
</code></pre><p>5.2 将创建好的对象丢至任务队列中：</p>
<pre><code>  // Submit the job
  JobStatus status;
  try {
    status = addJob(jobId, job);
  } catch (IOException ioe) {
    LOG.info(&quot;Job &quot; + jobId + &quot; submission failed!&quot;, ioe);
    status = job.getStatus();
    status.setFailureInfo(StringUtils.stringifyException(ioe));
    failJob(job);
    throw ioe;
  }
  return status;
}
}
</code></pre><p>addJob是JobTracker的一个方法：</p>
<pre><code>private synchronized JobStatus addJob(JobID jobId, JobInProgress job) throws IOException {
totalSubmissions++;
synchronized (jobs) {
  synchronized (taskScheduler) {
    jobs.put(job.getProfile().getJobID(), job);
    for (JobInProgressListener listener : jobInProgressListeners) {
      listener.jobAdded(job);
    }
  }
}
myInstrumentation.submitJob(job.getJobConf(), jobId);
job.getQueueMetrics().submitJob(job.getJobConf(), jobId);
LOG.info(&quot;Job &quot; + jobId + &quot; added successfully for user &apos;&quot; 
         + job.getJobConf().getUser() + &quot;&apos; to queue &apos;&quot; 
         + job.getJobConf().getQueueName() + &quot;&apos;&quot;);
AuditLogger.logSuccess(job.getUser(), 
    Operation.SUBMIT_JOB.name(), jobId.toString());
return job.getStatus();
}
</code></pre><p>核心部分就是下面的代码：</p>
<pre><code>synchronized (jobs) {
  synchronized (taskScheduler) {
    jobs.put(job.getProfile().getJobID(), job);
    for (JobInProgressListener listener : jobInProgressListeners) {
      listener.jobAdded(job);
    }
  }
}
</code></pre><p>jobs记录了JobTracker目前所有的作业：</p>
<pre><code>// All the known jobs.  (jobid-&gt;JobInProgress)
Map&lt;JobID, JobInProgress&gt; jobs =  
Collections.synchronizedMap(new TreeMap&lt;JobID, JobInProgress&gt;());
</code></pre><p>但这个对象并不是作业队列，真正的队列是jobInProgressListeners，该对象是一个JobInProgressListener的队列：</p>
<pre><code>private final List&lt;JobInProgressListener&gt; jobInProgressListeners =
new CopyOnWriteArrayList&lt;JobInProgressListener&gt;();
</code></pre><p>而JobInProgressListener是一个抽象类 ，其实现有很多个，如JobQueueJobInProgressListener用于监控job的运行状态，EagerTaskInitializationListener用于对Job进行初始化，JobTracker的JobInProgressListener队列里包含了多个这种类，新到的Job都被加入到各个JobInProgressListener中，以EagerTaskInitializationListener为例：</p>
<pre><code>/**
 * We add the JIP to the jobInitQueue, which is processed 
 * asynchronously to handle split-computation and build up
 * the right TaskTracker/Block mapping.
 */
@Override
public void jobAdded(JobInProgress job) {
  synchronized (jobInitQueue) {
    jobInitQueue.add(job);
    resortInitQueue();
    jobInitQueue.notifyAll();
  }
}
</code></pre><p>可见，Job被加入到队列中，jobInitQueue是一个List对象：</p>
<pre><code>  private List&lt;JobInProgress&gt; jobInitQueue = new ArrayList&lt;JobInProgress&gt;();
resortInitQueue()方法根据优先级等对队列里面的作业进行重新排列:
  /**
   * Sort jobs by priority and then by start time.
   */
  private synchronized void resortInitQueue() {
    Comparator&lt;JobInProgress&gt; comp = new Comparator&lt;JobInProgress&gt;() {
      public int compare(JobInProgress o1, JobInProgress o2) {
        int res = o1.getPriority().compareTo(o2.getPriority());
        if(res == 0) {
          if(o1.getStartTime() &lt; o2.getStartTime())
            res = -1;
          else
            res = (o1.getStartTime()==o2.getStartTime() ? 0 : 1);
        }

        return res;
      }
    };

    synchronized (jobInitQueue) {
      Collections.sort(jobInitQueue, comp);
    }
  }
</code></pre><p>因此，后加入队列的作业可能会因为优先级而被调整到前面执行。<br>jobInitQueue.notifyAll()这一行是通知其它线程，实际上是起到唤醒Job处理线程的作用，因为Job处理线程在没有Job的时候会wait，这可以从下面代码中看出：</p>
<pre><code>class JobInitManager implements Runnable {

  public void run() {
    JobInProgress job = null;
    while (true) {
      try {
        synchronized (jobInitQueue) {
          while (jobInitQueue.isEmpty()) {
            jobInitQueue.wait();
          }
          job = jobInitQueue.remove(0);
        }
        threadPool.execute(new InitJob(job));
      } catch (InterruptedException t) {
        LOG.info(&quot;JobInitManagerThread interrupted.&quot;);
        break;
      } 
    }
    LOG.info(&quot;Shutting down thread pool&quot;);
    threadPool.shutdownNow();
  }
}
</code></pre><p>当被唤醒后，会执行 job = jobInitQueue.remove(0)获得队列中第一个Job，并调用线程池，threadPool用JAVA中标准的线程池类java.util.concurrent.ExecutorService实现。<br>5.3 Job任务执行<br>从Job队列中取出Job任务后，创建了InitJob对象，丢入线程池执行时，则执行run方法：</p>
<pre><code>class InitJob implements Runnable {

  private JobInProgress job;

  public InitJob(JobInProgress job) {
    this.job = job;
  }

  public void run() {
    ttm.initJob(job);
  }
}
</code></pre><p>于是，进入ttm的intiJob方法，ttm是TaskTrackerManager对象，而TaskTrackerManager是一个接口，实现类只有JobTracker，因此实际上进入JobTracker的initJob方法执行：</p>
<pre><code>  public void initJob(JobInProgress job) {
 。。。。。。。try {
      JobStatus prevStatus = (JobStatus)job.getStatus().clone();
      LOG.info(&quot;Initializing &quot; + job.getJobID());
      job.initTasks();
      // Inform the listeners if the job state has changed
      JobStatus newStatus = (JobStatus)job.getStatus().clone();
      if (prevStatus.getRunState() != newStatus.getRunState()) {
        JobStatusChangeEvent event = 
          new JobStatusChangeEvent(job, EventType.RUN_STATE_CHANGED, prevStatus, 
              newStatus);
        synchronized (JobTracker.this) {
          updateJobInProgressListeners(event);
        }
      }
    } 
    }
。。。。。。
     }
</code></pre><p>之后，进入JobInProgress的initTasks方法执行作业的初始化。至此为止，JobClient作业提交、JobTracker将作业丢入队列、额外线程从队列中取出作业并丢到线程池中执行作业初始化的基本代码解析完毕。关于Job如何进行初始化，以及任务如何分配等等内容，我们留作后续博文中研究。<br>另外，上面我们遗留了一个问题，即Map任务的数量的确定。<br>6、Map数量的确定<br>在前面JobClient进行作业提交时，涉及到向配置文件job.xml中写入Map数量的代码：</p>
<pre><code>// Create the splits for the job
FileSystem fs = submitJobDir.getFileSystem(jobCopy);
LOG.debug(&quot;Creating splits at &quot; + fs.makeQualified(submitJobDir));
int maps = writeSplits(context, submitJobDir);
jobCopy.setNumMapTasks(maps);
</code></pre><p>writeSplits调用writeNewSplits，该方法最核心的是input.getSplits方法：</p>
<pre><code>private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,
    Path jobSubmitDir) throws IOException,
    InterruptedException, ClassNotFoundException {
  JobConf jConf = (JobConf)job.getConfiguration();
  int maps;
  if (jConf.getUseNewMapper()) {
    maps = writeNewSplits(job, jobSubmitDir);
  } else {
    maps = writeOldSplits(jConf, jobSubmitDir);
  }
  return maps;
}
private &lt;T extends InputSplit&gt;
int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,
    InterruptedException, ClassNotFoundException {
  Configuration conf = job.getConfiguration();
  InputFormat&lt;?, ?&gt; input =
    ReflectionUtils.newInstance(job.getInputFormatClass(), conf);
  List&lt;InputSplit&gt; splits = input.getSplits(job);
  T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);
  // sort the splits into order based on size, so that the biggest
  // go first
  Arrays.sort(array, new SplitComparator());
  JobSplitWriter.createSplitFiles(jobSubmitDir, conf,
      jobSubmitDir.getFileSystem(conf), array);
  return array.length;
}
</code></pre><p>createSplitFiles方法会生成两个文件：job.split和job.splitmetainfo：</p>
<pre><code>public static &lt;T extends InputSplit&gt; void createSplitFiles(Path jobSubmitDir, 
    Configuration conf, FileSystem fs, T[] splits) 
throws IOException, InterruptedException {
  FSDataOutputStream out = createFile(fs, 
      JobSubmissionFiles.getJobSplitFile(jobSubmitDir), conf);
  SplitMetaInfo[] info = writeNewSplits(conf, splits, out);
  out.close();
  writeJobSplitMetaInfo(fs,JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir), 
      new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION), splitVersion,
      info);
}
public static Path getJobSplitFile(Path jobSubmissionDir) {
  return new Path(jobSubmissionDir, &quot;job.split&quot;);
}
public static Path getJobSplitMetaFile(Path jobSubmissionDir) {
  return new Path(jobSubmissionDir, &quot;job.splitmetainfo&quot;);
}
</code></pre><p>在理解上述代码时，需要理解Split。<br>Split代表了对输入数据的一种逻辑分割，对于每一个分割，最终都会有一个Map任务进行计算，而Reduce的数量是用户指定的，因此Split决定了MapReduce的计算任务数量。<br>在所有的InputSplit里，最重要的是FileSplit，代表了对输入HDFS文件的分割：</p>
<pre><code>public class FileSplit extends InputSplit implements Writable {
  private Path file;
  private long start;
  private long length;
  private String[] hosts;
.......
</code></pre><p>从FileSplit的定义来看，主要记录了Split块所属文件、在文件中的起始位置，长度等。比如一个100T的大文件，分割成100个FileSplit，每个1T。只有确定了文件的分割方式，才能确定Map的数量。<br>因为输入数据可能是文件、HBase表等等，所以针对不同的输入数据格式，有不同的类实现这一Split分割功能。这里只从文件来看，由FileInputFormat实现分割。其核心方法为getSplits：</p>
<pre><code>/** 
 * Generate the list of files and make them into FileSplits.
 */ 
public List&lt;InputSplit&gt; getSplits(JobContext job
                                  ) throws IOException {
  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
  long maxSize = getMaxSplitSize(job);
  // generate splits
  List&lt;InputSplit&gt; splits = new ArrayList&lt;InputSplit&gt;();
  List&lt;FileStatus&gt;files = listStatus(job);
  for (FileStatus file: files) {
    Path path = file.getPath();
    FileSystem fs = path.getFileSystem(job.getConfiguration());
    long length = file.getLen();
    BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0, length);
    if ((length != 0) &amp;&amp; isSplitable(job, path)) { 
      long blockSize = file.getBlockSize();
      long splitSize = computeSplitSize(blockSize, minSize, maxSize);
      long bytesRemaining = length;
      while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) {
        int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
        splits.add(new FileSplit(path, length-bytesRemaining, splitSize, 
                                 blkLocations[blkIndex].getHosts()));
        bytesRemaining -= splitSize;
      }

      if (bytesRemaining != 0) {
        splits.add(new FileSplit(path, length-bytesRemaining, bytesRemaining, 
                   blkLocations[blkLocations.length-1].getHosts()));
      }
    } else if (length != 0) {
      splits.add(new FileSplit(path, 0, length, blkLocations[0].getHosts()));
    } else { 
      //Create empty hosts array for zero length files
      splits.add(new FileSplit(path, 0, length, new String[0]));
    }
  }

  // Save the number of input files in the job-conf
  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
  LOG.debug(&quot;Total # of splits: &quot; + splits.size());
  return splits;
}
</code></pre><p>上面的获取Split的方法中，核心步骤是：</p>
<pre><code>long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
long maxSize = getMaxSplitSize(job);
</code></pre><p>minSize为最小的Split大小，getFormatMinSplitSize的值是1，getMinSplitSize则来自于配置文件：</p>
<pre><code>/**
 * Get the lower bound on split size imposed by the format.
 * @return the number of bytes of the minimal split for this format
 */
protected long getFormatMinSplitSize() {
  return 1;
}
/**
 * Get the minimum split size
 * @param job the job
 * @return the minimum number of bytes that can be in a split
 */
public static long getMinSplitSize(JobContext job) {
  return job.getConfiguration().getLong(&quot;mapred.min.split.size&quot;, 1L);
}
</code></pre><p>getMaxSplitSize同样来自于配置文件，默认是Long的最大值：</p>
<pre><code>/**
 * Get the maximum split size.
 * @param context the job to look at.
 * @return the maximum number of bytes a split can include
 */
public static long getMaxSplitSize(JobContext context) {
  return context.getConfiguration().getLong(&quot;mapred.max.split.size&quot;, 
                                            Long.MAX_VALUE);
}
</code></pre><p>之后，获取文件信息：</p>
<pre><code>long length = file.getLen();
BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0, length);
</code></pre><p>获得文件的长度，并获取文件块所在的位置。BlockLocation记录了一个块在哪些DataNode机器上，其定义为：</p>
<pre><code>public class BlockLocation implements Writable {
  private String[] hosts; //hostnames of datanodes
  private String[] names; //hostname:portNumber of datanodes
  private String[] topologyPaths; // full path name in network topology
  private long offset;  //offset of the of the block in the file
  private long length;
。。。。。。。
  long blockSize = file.getBlockSize();
  long splitSize = computeSplitSize(blockSize, minSize, maxSize);
</code></pre><p>BlockSize为HDFS的块大小，默认64MB或128MB，splitSize则为Split的大小，computeSplitSize的实现为：</p>
<pre><code>protected long computeSplitSize(long blockSize, long minSize,
                                long maxSize) {
  return Math.max(minSize, Math.min(maxSize, blockSize));
}
</code></pre><p>可见，就是几个值选择一个。默认的，Split的大小就是blockSize，是一个Block的大小，也就是64MB等，当然，也可以设置为其它值。但无论如何设置，从下面的代码可以看出，Split的大小是一样的，除了最后一个FileSplit。而由于SPLIT_SLOP = 1.1，最后一个FileSplit是有可能大于一个Block大小的（&lt;1.1个BlockSize即可，默认情况）。</p>
<pre><code>long bytesRemaining = length;
while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) {
      int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
      splits.add(new FileSplit(path, length-bytesRemaining, splitSize,                                blkLocations[blkIndex].getHosts()));
      bytesRemaining -= splitSize;
    }
    //最后一个块
    if (bytesRemaining != 0) {
      splits.add(new FileSplit(path, length-bytesRemaining, bytesRemaining, 
                 blkLocations[blkLocations.length-1].getHosts()));
    }
</code></pre><p>每个FileSplit创建时，其大小都为splitSize，而这个值是通过前面代码获取的。从上面的代码中，blkInde表示某一个Block，而 blkLocations[blkIndex].getHosts()表示该Block所在的机器，这些机器作为FileSplit的参数，因此，应该是暗指Split的大小最好不要过大，比如假定一个Split是2个Block大小，而这个2个Block如果位于两个服务器上，那么在构造FileSplit时，则最终分配的Map任务只会位于一台服务器上，另一个Block则需要通过网络传输至Map所在机器，这说明，SPlit不宜大，最好保持就默认为一个Block。<br>从其构造函数来看：</p>
<pre><code>public FileSplit(Path file, long start, long length, String[] hosts) {
this.file = file;
this.start = start;
this.length = length;
this.hosts = hosts;
}
</code></pre><p>假如length很大，跨越了很多个Block，则可能跨越多台服务器，但hosts只是记录了某一个Block所处的服务器。从下面的方法可以看出，给Split分配的服务器就是Split的起始位置所在Block的所在机器（当然按照3份备份来说，至少有3台）。</p>
<pre><code>protected int getBlockIndex(BlockLocation[] blkLocations, 
                            long offset) {
  for (int i = 0 ; i &lt; blkLocations.length; i++) {
    // is the offset inside this block?
    if ((blkLocations[i].getOffset() &lt;= offset) &amp;&amp;
        (offset &lt; blkLocations[i].getOffset() + blkLocations[i].getLength())){
      return i;
    }
  }
  BlockLocation last = blkLocations[blkLocations.length -1];
  long fileLength = last.getOffset() + last.getLength() -1;
  throw new IllegalArgumentException(&quot;Offset &quot; + offset + 
                                     &quot; is outside of file (0..&quot; +
                                     fileLength + &quot;)&quot;);
}
</code></pre><p>FileInputFormat用于获取输入数据的分割，该类继承于基类InputFormat<k, v="">，其定义为：</k,></p>
<pre><code>public abstract class InputFormat&lt;K, V&gt; {
  /** 
   * Logically split the set of input files for the job.  
   * 
   * &lt;p&gt;Each {@link InputSplit} is then assigned to an individual {@link Mapper}
   * for processing.&lt;/p&gt;
   *
   * &lt;p&gt;&lt;i&gt;Note&lt;/i&gt;: The split is a &lt;i&gt;logical&lt;/i&gt; split of the inputs and the
   * input files are not physically split into chunks. For e.g. a split could
   * be &lt;i&gt;&amp;lt;input-file-path, start, offset&amp;gt;&lt;/i&gt; tuple. The InputFormat
   * also creates the {@link RecordReader} to read the {@link InputSplit}.
   * 
   * @param context job configuration.
   * @return an array of {@link InputSplit}s for the job.
   */
  public abstract 
    List&lt;InputSplit&gt; getSplits(JobContext context
                               ) throws IOException, InterruptedException;

  /**
   * Create a record reader for a given split. The framework will call
   * {@link RecordReader#initialize(InputSplit, TaskAttemptContext)} before
   * the split is used.
   * @param split the split to be read
   * @param context the information about the task
   * @return a new record reader
   * @throws IOException
   * @throws InterruptedException
   */
  public abstract 
    RecordReader&lt;K,V&gt; createRecordReader(InputSplit split,
                                         TaskAttemptContext context
                                        ) throws IOException, 
                                                 InterruptedException;
    }
</code></pre><p>里面必须实现两个方法：getSplits、createRecordReader。因此，getSplits获得对输入数据的分割方式，createRecordReader则返回一个可以读取记录的类。<br>从前面可以看出，如果采用默认形式，则Map数量等于Block的数量，一般情况下，Split尺寸就等于Block。这里可能就会有个疑问，某些记录较长（比如一行文本），可能会跨越多个Block，那么，也就会跨越多个Split，而Map和Split是一一对应关系，跨越边界的记录被哪个Map执行呢？这个问题由RecordReader保证，在处理文本文件时，Hadoop提供了一些基本实现，典型的有TextInputFormat，这个类继承于FileInputFormat<longwritable, text=""> ，其声明为：</longwritable,></p>
<pre><code>public class TextInputFormat extends FileInputFormat&lt;LongWritable, Text&gt; {
  @Override
  public RecordReader&lt;LongWritable, Text&gt; 
    createRecordReader(InputSplit split,
                       TaskAttemptContext context) {
    return new LineRecordReader();
  }
  @Override
  protected boolean isSplitable(JobContext context, Path file) {
    CompressionCodec codec = 
      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    if (null == codec) {
      return true;
    }
    return codec instanceof SplittableCompressionCodec;
  }
}
</code></pre><p>可见，其实现了LineRecordReader，读取整行记录。如果一行文本跨越了多个Split，则LineRecordReader不会关心SPlit，而会保证跨越的那个文本行由前一个Map任务执行，其代码为：</p>
<pre><code>public boolean nextKeyValue() throws IOException {
  if (key == null) {
    key = new LongWritable();
  }
  key.set(pos);
  if (value == null) {
    value = new Text();
  }
  int newSize = 0;
  // We always read one extra line, which lies outside the upper
  // split limit i.e. (end - 1)
  while (getFilePosition() &lt;= end) {
    newSize = in.readLine(value, maxLineLength,
        Math.max(maxBytesToConsume(pos), maxLineLength));
    if (newSize == 0) {
      break;
    }
    pos += newSize;
    if (newSize &lt; maxLineLength) {
      break;
    }
    // line too long. try again
    LOG.info(&quot;Skipped line of size &quot; + newSize + &quot; at pos &quot; + 
             (pos - newSize));
  }
  if (newSize == 0) {
    key = null;
    value = null;
    return false;
  } else {
    return true;
  }
}
</code></pre><p>其readLine方法会读取一整行，因为这是针对HDFS操作的，所以不管这一行是否跨越了多个Split，都会把记录全部读进来处理。<br>而在LineRecordReader的initialize方法中，有以下关键代码：</p>
<pre><code>  public void initialize(InputSplit genericSplit,
                         TaskAttemptContext context) throws IOException {
    FileSplit split = (FileSplit) genericSplit;
    Configuration job = context.getConfiguration();
    this.maxLineLength = job.getInt(&quot;mapred.linerecordreader.maxlength&quot;,
                                    Integer.MAX_VALUE);
    start = split.getStart();
    end = start + split.getLength();
    final Path file = split.getPath();
.....
</code></pre><p>start和end记录了该Split在整个文件中的起始位置和结束位置。并跳到Split的起始位置：</p>
<pre><code>fileIn.seek(start);
in = new LineReader(fileIn, job);
filePosition = fileIn;
</code></pre><p>之后，如果start不为0，表明这不是第一个Split块，则会直接读取一行，而并不进行处理：</p>
<pre><code>// If this is not the first split, we always throw away first record
// because we always (except the last split) read one extra line in
// next() method.
if (start != 0) {
  start += in.readLine(new Text(), 0, maxBytesToConsume(start));
}
this.pos = start;
</code></pre><p>这样就相当于忽略了跨越Split边界第一行，因为这一行已经被上一个Split对应的Map任务处理了。<br>总的来说，Map任务数量决定于Split数量，一般等于HDFS Block的整数倍，会存在一条记录跨越多个Split的情况，记录读取由RecordReader的实现类决定，在这个类中，会处理好边界问题。</p>
<hr>
<p>后记：<br>Job提交的过程基本剖析完毕，从官网摘出其流程图：<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia4.jpg" alt="官网流程图"><br>可以看出，一共有10个基本步骤：<br>1、创建JobClient对象，实现与JobTracker的RPC访问；<br>2、获取Job ID；<br>3、向HDFS上传所需数据文件（不是HDFS文件数据）、资源文件；这些文件属于Job相关文件，Map任务数量由JobClient负责统计得到，生成后写入HDFS的job.split文件中；<br>4、提交Job至JobTracker的队列；<br>5、初始化，即从Job队列中取出来进行初始化操作；<br>6、获取job.split等文件，进行任务分配等操作；<br>7、TaskTracker通过心跳机制，获取Job；<br>8、TaskTracker从HDFS中获取所需jar包、参数等资源信息；<br>9、启动虚拟机；<br>10、Map、Reduce Task在虚拟机中执行。<br>在本博文中，对1-4的过程进行了分析，关于JobTracker如何取出Job，以及Job如何分配，TaskTracker如何获取Map/Reduce Task的细节在后续博文中进行分析。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MapReduce框架详解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/10/03/MapReduce框架详解/" class="article-date">
  	<time datetime="2015-10-03T11:33:02.000Z" itemprop="datePublished">2015-10-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/03/MapReduce框架详解/">MapReduce框架详解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="MapReduce_u6846_u67B6_u8BE6_u89E3"><a href="#MapReduce_u6846_u67B6_u8BE6_u89E3" class="headerlink" title="MapReduce框架详解"></a>MapReduce框架详解</h2><blockquote>
<p>我现在学习技术很喜欢看图，每次有了新理解就会去看看图，每次都会有新的发现。<br>谈mapreduce运行机制，可以从很多不同的角度来描述，比如说从mapreduce运行流程来讲解，也可以从计算模型的逻辑流程来进行讲解，也许有些深入理解了mapreduce运行机制还会从更好的角度来描述，但是将mapreduce运行机制有些东西是避免不了的，就是一个个参入的实例对象，一个就是计算模型的逻辑定义阶段，我这里讲解不从什么流程出发，就从这些一个个牵涉的对象，不管是物理实体还是逻辑实体。<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia1.jpg" alt="MapReduce1"><br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia2.jpg" alt="MapReduce2"><br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia3.jpg" alt="MapReduce3"><br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia4.jpg" alt="MapReduce4"><br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Kuangjia5.jpg" alt="MapReduce5"><br>下面介绍MapReduce的部分参数及其默认设置：<br>（1）InputFormat类<br>该类的作用是将输入的数据分割成一个个的split，并将split进一步拆分成<key, value="">对作为map函数的输入<br>（2）Mapper类<br>实现map函数，根据输入的<key, value="">对生产中间结果<br>（3）Combiner<br>实现combine函数，合并中间结果中具有相同key值的键值对。<br>（4）Partitioner类<br>实现getPartition函数，用于在Shuffle过程按照key值将中间数据分成R份，每一份由一个Reduce负责<br>（5）Reducer类<br>实现reduce函数，将中间结果合并，得到最终的结果<br>（6）OutputFormat类<br>该类负责输出最终的结果<br>首先讲讲物理实体，参入mapreduce作业执行涉及4个独立的实体：<br>（1）客户端（client）：编写mapreduce程序，配置作业，提交作业，这就是程序员完成的工作；<br>（2）JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行；<br>（3）TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的）<br>（4）Hdfs：保存作业的数据、配置信息等等，最后的结果也是保存在hdfs上面<br>那么mapreduce到底是如何运行的呢？<br>首先是客户端要编写好mapreduce程序，配置好mapreduce的作业也就是job，接下来就是提交job了，提交job是提交到JobTracker上的，这个时候JobTracker就会构建这个job，具体就是分配一个新的job任务的ID值，接下来它会做检查操作，这个检查就是确定输出目录是否存在，如果存在那么job就不能正常运行下去，JobTracker会抛出错误给客户端，接下来还要检查输入目录是否存在，如果不存在同样抛出错误，如果存在JobTracker会根据输入计算输入分片（Input Split），如果分片计算不出来也会抛出错误，至于输入分片我后面会做讲解的，这些都做好了JobTracker就会配置Job需要的资源了。分配好资源后，JobTracker就会初始化作业，初始化主要做的是将Job放入一个内部的队列，让配置好的作业调度器能调度到这个作业，作业调度器会初始化这个job，初始化就是创建一个正在运行的job对象（封装任务和记录信息），以便JobTracker跟踪job的状态和进程。<br>初始化完毕后，作业调度器会获取输入分片信息（input split），每个分片创建一个map任务。接下来就是任务分配了，这个时候tasktracker会运行一个简单的循环机制定期发送心跳给jobtracker，心跳间隔是5秒，程序员可以配置这个时间，心跳就是jobtracker和tasktracker沟通的桥梁，通过心跳，jobtracker可以监控tasktracker是否存活，也可以获取tasktracker处理的状态和问题，同时tasktracker也可以通过心跳里的返回值获取jobtracker给它的操作指令。任务分配好后就是执行任务了。在任务执行时候jobtracker可以通过心跳机制监控tasktracker的状态和进度，同时也能计算出整个job的状态和进度，而tasktracker也可以本地监控自己的状态和进度。当jobtracker获得了最后一个完成指定任务的tasktracker操作成功的通知时候，jobtracker会把整个job状态置为成功，然后当客户端查询job运行状态时候（注意：这个是异步操作），客户端会查到job完成的通知的。如果job中途失败，mapreduce也会有相应机制处理，一般而言如果不是程序员程序本身有bug，mapreduce错误处理机制都能保证提交的job能正常完成。</key,></key,></p>
</blockquote>
<hr>
<blockquote>
<p>下面我从逻辑实体的角度讲解mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。<br>（1）. 输入分片（input split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。</p>
<ol>
<li>map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行；</li>
<li>combiner阶段：combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。</li>
<li>shuffle阶段：将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。<br> 每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</li>
<li>reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。<h4 id="Mapreduce_u7684_u76F8_u5173_u95EE_u9898"><a href="#Mapreduce_u7684_u76F8_u5173_u95EE_u9898" class="headerlink" title="Mapreduce的相关问题"></a>Mapreduce的相关问题</h4>这里我要谈谈我学习mapreduce思考的一些问题，都是我自己想出解释的问题，但是某些问题到底对不对，就要广大童鞋帮我确认了。<br>（1）jobtracker的单点故障：jobtracker和hdfs的namenode一样也存在单点故障，单点故障一直是hadoop被人诟病的大问题，为什么hadoop的做的文件系统和mapreduce计算框架都是高容错的，但是最重要的管理节点的故障机制却如此不好，我认为主要是namenode和jobtracker在实际运行中都是在内存操作，而做到内存的容错就比较复杂了，只有当内存数据被持久化后容错才好做，namenode和jobtracker都可以备份自己持久化的文件，但是这个持久化都会有延迟，因此真的出故障，任然不能整体恢复，另外hadoop框架里包含zookeeper框架，zookeeper可以结合jobtracker，用几台机器同时部署jobtracker，保证一台出故障，有一台马上能补充上，不过这种方式也没法恢复正在跑的mapreduce任务。<br>（2）做mapreduce计算时候，输出一般是一个文件夹，而且该文件夹是不能存在，我在出面试题时候提到了这个问题，而且这个检查做的很早，当我们提交job时候就会进行，mapreduce之所以这么设计是保证数据可靠性，如果输出目录存在reduce就搞不清楚你到底是要追加还是覆盖，不管是追加和覆盖操作都会有可能导致最终结果出问题，mapreduce是做海量数据计算，一个生产计算的成本很高，例如一个job完全执行完可能要几个小时，因此一切影响错误的情况mapreduce是零容忍的。<br>（3） Mapreduce还有一个InputFormat和OutputFormat，我们在编写map函数时候发现map方法的参数是之间操作行数据，没有牵涉到InputFormat，这些事情在我们new Path时候mapreduce计算框架帮我们做好了，而OutputFormat也是reduce帮我们做好了，我们使用什么样的输入文件，就要调用什么样的InputFormat，InputFormat是和我们输入的文件类型相关的，mapreduce里常用的InputFormat有FileInputFormat普通文本文件，SequenceFileInputFormat是指hadoop的序列化文件，另外还有KeyValueTextInputFormat。OutputFormat就是我们想最终存储到hdfs系统上的文件格式了，这个根据你需要定义了，hadoop有支持很多文件格式，这里不一一列举，想知道百度下就看到了。</li>
</ol>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MapReduce实例浅析" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/10/02/MapReduce实例浅析/" class="article-date">
  	<time datetime="2015-10-02T07:23:12.000Z" itemprop="datePublished">2015-10-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/02/MapReduce实例浅析/">MapReduce实例浅析</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="MapReduce_u5B9E_u4F8B_u6D45_u6790"><a href="#MapReduce_u5B9E_u4F8B_u6D45_u6790" class="headerlink" title="MapReduce实例浅析"></a>MapReduce实例浅析</h2><h4 id="1-MapReduce_u6982_u8FF0"><a href="#1-MapReduce_u6982_u8FF0" class="headerlink" title="1.MapReduce概述"></a>1.MapReduce概述</h4><blockquote>
<p>Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。<br>    一个Map/Reduce 作业（job） 通常会把输入的数据集切分为若干独立的数据块，由 map任务（task）以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。<br>    通常，Map/Reduce框架和分布式文件系统是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。<br>    Map/Reduce框架由一个单独的master JobTracker 和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。<br>    应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop的 job client提交作业（jar包/可执行程序等）和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。<br>虽然Hadoop框架是用Java实现的，但Map/Reduce应用程序则不一定要用 Java来写 。<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Shili1.jpg" alt="MapReduce处理大数据过程"></p>
</blockquote>
<h4 id="2-_u6837_u4F8B_u5206_u6790_uFF1A_u5355_u8BCD_u8BA1_u6570"><a href="#2-_u6837_u4F8B_u5206_u6790_uFF1A_u5355_u8BCD_u8BA1_u6570" class="headerlink" title="2.样例分析：单词计数"></a>2.样例分析：单词计数</h4><blockquote>
<h5 id="WordCount_u6E90_u7801_u5206_u6790"><a href="#WordCount_u6E90_u7801_u5206_u6790" class="headerlink" title="WordCount源码分析"></a>WordCount源码分析</h5><p>单词计数是最简单也是最能体现MapReduce思想的程序之一，该程序完整的代码可以在Hadoop安装包的src/examples目录下找到<br>单词计数主要完成的功能是：统计一系列文本文件中每个单词出现的次数，如图所示：<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce%26%26Shili2.jpg" alt="单词计数"><br>（1）    Map过程<br>Map过程需要继承org.apache.hadoop.mapreduce包中的Mapper类，并重写map方法<br>通过在map方法中添加两句把key值和value值输出到控制台的代码，可以发现map方法中的value值存储的是文本文件中的一行（以回车符作为行结束标记），而key值为该行的首字符相对于文本文件的首地址的偏移量。然后StringTokenizer类将每一行拆分成一个个的单词，并将<word,1>作为map方法的结果输出，其余的工作都交由MapReduce框架处理。其中IntWritable和Text类是Hadoop对int和string类的封装，这些类能够被串行化，以方便在分布式环境中进行数据交换。</word,1></p>
<p>TokenizerMapper的实现代码如下：</p>
</blockquote>
<pre><code>public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
    System.out.println(&quot;key = &quot; + key.toString());
    //添加查看key值
    System.out.println(&quot;value = &quot; + value.toString());
    //添加查看value值
    StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);
        }
    }
}
</code></pre><p>（2）    Reduce过程</p>
<blockquote>
<p>Reduce过程需要继承org.apache.hadoop.mapreduce包中的Reducer类，并重写reduce方法<br>reduce方法的输入参数key为单个单词，而values是由各Mapper上对应单词的计数值所组成的列表，所以只要遍历values并求和，即可得到某个单词的出现总次数<br>IntSumReduce类的实现代码如下：</p>
</blockquote>
<pre><code>public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
    System.out.println(&quot;key = &quot; + key.toString());
    //添加查看key值
    System.out.println(&quot;value = &quot; + value.toString());
    //添加查看value值
    StringTokenizer itr = new StringTokenizer(value.toString());
    while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
    }
}
</code></pre><p>}</p>
<p>（3）    执行MapReduce任务</p>
<blockquote>
<p>在MapReduce中，由Job对象负责管理和运行一个计算任务，并通过Job的一些方法对任务的参数进行相关的设置。此处设置了使用TokenizerMapper完成Map过程和使用的IntSumReduce完成Combine和Reduce过程。还设置了Map过程和Reduce过程的输出类型：key的类型为Text，value的类型为IntWritable。任务的输入和输出路径则由命令行参数指定，并由FileInputFormat和FileOutputFormat分别设定。完成相应任务的参数设定后，即可调用job.waitForCompletion()方法执行任务，主函数实现如下：</p>
</blockquote>
<pre><code>public static void main(String[] args) throws Exception {
Configuration conf = new Configuration();
String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
if (otherArgs.length != 2) {
  System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;);
  System.exit(2);
}
Job job = new Job(conf, &quot;word count&quot;);
job.setJarByClass(wordCount.class);
job.setMapperClass(TokenizerMapper.class);
job.setCombinerClass(IntSumReducer.class);
job.setReducerClass(IntSumReducer.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}
</code></pre><h5 id="WordCount_u5904_u7406_u8FC7_u7A0B"><a href="#WordCount_u5904_u7406_u8FC7_u7A0B" class="headerlink" title="WordCount处理过程"></a>WordCount处理过程</h5><blockquote>
<p>上面给出了WordCount的设计思路和源码，但是没有深入细节，下面对WordCount进行更加详细的分析：<br>1.将文件拆分成splits，由于测试用的文件较小，所以每一个文件为一个split，并将文件按行分割成<key, value="">对，如图，这一步由Mapreduce框架自动完成，其中偏移量包括了回车所占的字符<br>2.将分割好的<key, value="">对交给用户定义的map方法进行处理，生成新的<key, value="">对<br>3.得到map方法输出的<key, value="">对后，Mapper会将它们按照key值进行排序，并执行Combine过程，将key值相同的value值累加，得到Mapper的最终输出结果，如图：<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Shili3.jpg" alt="Reduce过程"><br>4.Reduce先对从Mapper接收的数据进行排序，再交由用户自定义的reduce方法进行处理，得到新的<key, value="">对，并作为WordCount的输出结果，如图：<br><img src="http://7xract.com1.z0.glb.clouddn.com/MapReduce&amp;&amp;Shili4.jpg" alt="Reduce输出结果"></key,></key,></key,></key,></key,></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-TCP窗口滑动以及拥塞控制" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/03/01/TCP窗口滑动以及拥塞控制/" class="article-date">
  	<time datetime="2015-03-01T02:13:21.000Z" itemprop="datePublished">2015-03-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/03/01/TCP窗口滑动以及拥塞控制/">TCP窗口滑动以及拥塞控制</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="TCP_u7A97_u53E3_u6ED1_u52A8_u4EE5_u53CA_u62E5_u585E_u63A7_u5236"><a href="#TCP_u7A97_u53E3_u6ED1_u52A8_u4EE5_u53CA_u62E5_u585E_u63A7_u5236" class="headerlink" title="TCP窗口滑动以及拥塞控制"></a>TCP窗口滑动以及拥塞控制</h2><p>TCP协议作为一个可靠的面向流的传输协议，其可靠性和流量控制由滑动窗口协议保证，而拥塞控制则由控制窗口结合一系列的控制算法实现。</p>
<blockquote>
<p>一、滑动窗口协议<br>     关于这部分自己不晓得怎么叙述才好，因为理解的部分更多，下面就用自己的理解来介绍下TCP的精髓：滑动窗口协议。<br>     所谓滑动窗口协议，自己理解有两点：1. “窗口”对应的是一段可以被发送者发送的字节序列，其连续的范围称之为“窗口”；2. “滑动”则是指这段“允许发送的范围”是可以随着发送的过程而变化的，方式就是按顺序“滑动”。在引入一个例子来说这个协议之前，我觉得很有必要先了解以下前提：<br>1）TCP协议的两端分别为发送者A和接收者B，由于是全双工协议，因此A和B应该分别维护着一个独立的发送缓冲区和接收缓冲区，由于对等性（A发B收和B发A收），我们以A发送B接收的情况作为例子；<br>2）发送窗口是发送缓存中的一部分，是可以被TCP协议发送的那部分，其实应用层需要发送的所有数据都被放进了发送者的发送缓冲区；<br>3）发送窗口中相关的有四个概念：已发送并收到确认的数据（不再发送窗口和发送缓冲区之内）、已发送但未收到确认的数据（位于发送窗口之中）、允许发送但尚未发送的数据以及发送窗口外发送缓冲区内暂时不允许发送的数据；<br>4）每次成功发送数据之后，发送窗口就会在发送缓冲区中按顺序移动，将新的数据包含到窗口中准备发送；<br>     TCP建立连接的初始，B会告诉A自己的接收窗口大小，比如为‘20’：<br>     字节31-50为发送窗口<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_1.png" alt="TCP窗口滑动以及拥塞控制_1"><br>A发送11个字节后，发送窗口位置不变，B接收到了乱序的数据分组：<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_2.png" alt="TCP窗口滑动以及拥塞控制_2"><br>只有当A成功发送了数据，即发送的数据得到了B的确认之后，才会移动滑动窗口离开已发送的数据；同时B则确认连续的数据分组，对于乱序的分组则先接收下来，避免网络重复传递：<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_3.png" alt="TCP窗口滑动以及拥塞控制_3"></p>
</blockquote>
<hr>
<blockquote>
<p>二、流量控制<br>     流量控制方面主要有两个要点需要掌握。一是TCP利用滑动窗口实现流量控制的机制；二是如何考虑流量控制中的传输效率。</p>
<ol>
<li>流量控制<br>  所谓流量控制，主要是接收方传递信息给发送方，使其不要发送数据太快，是一种端到端的控制。主要的方式就是返回的ACK中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送：<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_4.png" alt="TCP窗口滑动以及拥塞控制_4"><br>这里面涉及到一种情况，如果B已经告诉A自己的缓冲区已满，于是A停止发送数据；等待一段时间后，B的缓冲区出现了富余，于是给A发送报文告诉A我的rwnd大小为400，但是这个报文不幸丢失了，于是就出现A等待B的通知||B等待A发送数据的死锁状态。为了处理这种问题，TCP引入了持续计时器（Persistence timer），当A收到对方的零窗口通知时，就启用该计时器，时间到则发送一个1字节的探测报文，对方会在此时回应自身的接收窗口大小，如果结果仍未0，则重设持续计时器，继续等待。</li>
<li>传递效率<br>  一个显而易见的问题是：单个发送字节单个确认，和窗口有一个空余即通知发送方发送一个字节，无疑增加了网络中的许多不必要的报文（请想想为了一个字节数据而添加的40字节头部吧！），所以我们的原则是尽可能一次多发送几个字节，或者窗口空余较多的时候通知发送方一次发送多个字节。对于前者我们广泛使用Nagle算法，即：<br>1） 若发送应用进程要把发送的数据逐个字节地送到TCP的发送缓存，则发送方就把第一个数据字节先发送出去，把后面的字节先缓存起来；<br>2）当发送方收到第一个字节的确认后（也得到了网络情况和对方的接收窗口大小），再把缓冲区的剩余字节组成合适大小的报文发送出去；<br>3） 当到达的数据已达到发送窗口大小的一半或以达到报文段的最大长度时，就立即发送一个报文段；<br>  对于后者我们往往的做法是让接收方等待一段时间，或者接收方获得足够的空间容纳一个报文段或者等到接受缓存有一半空闲的时候，再通知发送方发送数据。</li>
</ol>
</blockquote>
<hr>
<blockquote>
<p>三、拥塞控制<br>     网络中的链路容量和交换结点中的缓存和处理机都有着工作的极限，当网络的需求超过它们的工作极限时，就出现了拥塞。拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。常用的方法就是：</p>
<ol>
<li>慢开始、拥塞控制</li>
<li>快重传、快恢复<br>  一切的基础还是慢开始，这种方法的思路是这样的：<br>1） 发送方维持一个叫做“拥塞窗口”的变量，该变量和接收端口共同决定了发送者的发送窗口；<br>2） 当主机开始发送数据时，避免一下子将大量字节注入到网络，造成或者增加拥塞，选择发送一个1字节的试探报文；<br>3）当收到第一个字节的数据的确认后，就发送2个字节的报文；<br>4） 若再次收到2个字节的确认，则发送4个字节，依次递增2的指数级；<br>5）最后会达到一个提前预设的“慢开始门限”，比如24，即一次发送了24个分组，此时遵循下面的条件判定：<br>cwnd &lt; ssthresh， 继续使用慢开始算法；<br>cwnd &gt; ssthresh，停止使用慢开始算法，改用拥塞避免算法；<br>cwnd = ssthresh，既可以使用慢开始算法，也可以使用拥塞避免算法；<br>6） 所谓拥塞避免算法就是：每经过一个往返时间RTT就把发送方的拥塞窗口+1，即让拥塞窗口缓慢地增大，按照线性规律增长；<br>7）当出现网络拥塞，比如丢包时，将慢开始门限设为原先的一半，然后将cwnd设为1，执行慢开始算法（较低的起点，指数级增长）；<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_5.png" alt="TCP窗口滑动以及拥塞控制_5"><br>上述方法的目的是在拥塞发生时循序减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够的时间把队列中积压的分组处理完毕。慢开始和拥塞控制算法常常作为一个整体使用，而快重传和快恢复则是为了减少因为拥塞导致的数据包丢失带来的重传时间，从而避免传递无用的数据到网络。快重传的机制是：<br>1） 接收方建立这样的机制，如果一个包丢失，则对后续的包继续发送针对该包的重传请求；<br>2） 一旦发送方接收到三个一样的确认，就知道该包之后出现了错误，立刻重传该包；<br>3）此时发送方开始执行“快恢复”算法：<br>慢开始门限减半；<br>cwnd设为慢开始门限减半后的数值；<br>执行拥塞避免算法（高起点，线性增长）；<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E4%BB%A5%E5%8F%8A%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6_6.png" alt="TCP窗口滑动以及拥塞控制_6"></li>
</ol>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/network/">network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-TCP连接的建立和终止" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/02/27/TCP连接的建立和终止/" class="article-date">
  	<time datetime="2015-02-27T03:34:10.000Z" itemprop="datePublished">2015-02-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/27/TCP连接的建立和终止/">TCP连接的建立和终止</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="TCP_u8FDE_u63A5_u7684_u5EFA_u7ACB_u548C_u7EC8_u6B62"><a href="#TCP_u8FDE_u63A5_u7684_u5EFA_u7ACB_u548C_u7EC8_u6B62" class="headerlink" title="TCP连接的建立和终止"></a>TCP连接的建立和终止</h2><blockquote>
<p>以下给出了一个基本TCP客户和服务器程序的套接口函数。<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_1.jpg" alt="TCP连接的建立和终止_1"><br>    先从服务器端说起。服务器端先调用socket函数(返回一个套接字)，然后套接字与地址、端口绑定(bind)，对端口进行监听（listen），调用accept阻塞，等待客户端连接。在这时如果有个客户端调用socket函数，然后连接服务器（connect），如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把响应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。<br>一、TCP包的首部<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_2.jpg" alt="TCP连接的建立和终止_2"><br>1)若不计选项字段，TCP的首部占20个字节。<br>2)源端口号以及目的端口号用于寻找发端和接收端的进程，一般来讲，通过端口号和IP地址，可以唯一确定一个TCP连接，在网络编程中，通常被称为一个socket接口。<br>3)序号是用来标识从TCP发端向TCP接收端发送的数据字节流。<br>4)确认序号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应该是上次已经成功收到数据字节序号加1.<br>5)首部长度指出了TCP首部的长度值，若不存在选项，则这个值为20字节。<br>6)下表给出了了TCP连接过程中的一些名词的意义。<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_3.png" alt="TCP连接的建立和终止_3"><br>TCP提供解决方式是为了使一端告诉另外一端某些“紧急数据”已经放置在普通的数据流中，让接收端对紧急数据做特别处理。此时，URG位被置为1，并且16位的紧急数据被置为一个正的偏移量，通过此偏移量与TCP首部中的序号字段相加，可以得出紧急数据的最后一个字节的序号，常见的应用有传输中断键（在通过telnet连接过程中）。<br>    RST: 复位字段被用于当一个报文发送到某个socket接口而出现错误时，TCP则会发出复位报文段。常见出现的情况有以下几种：<br>    发送到不存在的端口的连接请求：此时对方的目的端口并没有侦听，对于UDP，将会发出ICMP不可达的   错误信息，而对于TCP，将会发出设置RST复位标志位的数据报。异常终止一个连接：正常情况下，通过发送FIN去正常关闭一个TCP连接，但也有可能通过发送一个复位   报文段去中途释放掉一个连接。在socketAPI中通过设置socket选 项SO_LINGER去关闭这种异常关闭的情况。<br>二、三次握手建立连接详解<br>    我们知道TCP连接要进行“三次握手”，即交换三个分节。大致流程如下：<br>    1）客户端向服务器发送一个SYN J；<br>    2）服务器端想客户端响应一个SYN K，并对SYN J进行确认ACK J+1；<br>    3）客户端再向服务器发一个确认ACK K+1.<br>    这样就完成了三次握手，但是这个三次握手发生在套接口的哪几个函数呢？请看下图：<br>    <img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_4.png" alt="TCP连接的建立和终止_4"><br> 从图中可以看出，服务器必须准备好接受外来的连接。这通过socket、bind和listen函数来完成，称为被动打开。<br>    客户端通过调用connect进行主动打开。这引起客户端向服务器发送SYN J（表示同步，它告诉服务器将在连接中发送的数据的初始序列号）分节，这时connect进入阻塞状态。<br>    服务器监听到连接请求，即收到SYN J分节，调用accept函数接受请求，并向客户端发送SYN K（它告诉客户端服务器将在连接中发送的数据的初始化序列号）、ACK J+1分节，这时accept进入阻塞状态。<br>    客户端收到服务器端的SYN K、ACK J+1分节后，这时connect返回，并对SYN K分节进行确认；服务器接收到ACK K+1分节时，accept的返回，至此三次握手完毕，连接建立。<br>    总结：客户端的connect在三次握手的第二次返回，而服务器端的accept在三次握手的第三次返回。<br>    问题：TCP为什么不采用二次握手而采用三次握手？<br>    采用三次握手是为了防止失效的连接请求报文突然又传到服务器，从而发生错误。当客户发出的里阿尼额请求由于某些原因没有及时到达服务器，而客户在等待一段时间后重新向服务器发送连接请求，且建立成功，顺序完成数据传输，那么第一次发送的连接请求报文段称为失效的连接请求报文段。<br>    考虑这样一种特殊情况，客户端第一次发送的连接请求并没有丢失，而是因为网络问题导致延迟到达服务器，服务器以为是客户端有发起的新连接，于是连接服务器统一连接，并向服务端发回确认，但是此时客户端不予理会，服务器就一直等待客户发送数据，导致服务器的资源浪费。<br>三、四次挥手释放连接详解<br> 建立一个连接需要三次握手，而终止一个连接要经过4次挥手。由于TCP的半封闭（half-close）造成的。既然一个TCP连接是全双工（即数据在两个方向是哪个能同时传递），因此每个方向必须单独地进行关闭。现在我们接受TCP的四次挥手释放连接过程，如下图所示：<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_5.jpg" alt="TCP连接的建立和终止_5"><br>具体步骤：<br>1）某个应用进程首先调用close，我们称这一端执行主动关闭。这一端的TCP于是发送一个FIN分节，表示数据发送完毕；<br>2）另一端接收到FIN分节之后，执行被动关闭，对这个FIN进行确认。它的接收也作为文件结束符传递给接收端应用进程，因为FIN的接收意味着应用进程在相应的连接上再也接收不到额外数据；<br>3）一段时间之后，接收到文件结束符的应用进程调用close关闭它的套接口。这导致它 的TCP也发送一个FIN：<br>4）接收到这个FIN的原发送端TCP（即执行主动关闭的那一端）对它进行确认。<br>    这样每个方向上都有一个FIN和ACK，所以一共需要四个分节。我们使用限定词“一般”是因为：优势步骤1）的FIN随数据一起发送；另外，执行被动关闭那一端的TCP在步骤2）和步骤3）发出的ACK与FIN也可以合并成一个分节。<br>    TCP关闭时，每一端都要发送一个FIN。这种情况除了在应用进程调用close时会发生，还会在进程终止时发生。进程终止包括自愿（调用exit或从main函数返回）、不自愿（进程收到一个终止本进程的信号）的情况，进程终止时所有打开的TCP连接都会发出一个FIN。<br>    上图是客户端执行主动关闭，然而不管是客户端还是服务器端都可以执行主动关闭。通常情况下式客户端主动关闭，但某次额协议如HTTP则是服务端执行主动关闭。<br>    问题：TCP为何采用四次挥手来释放连接。<br>    关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方内有数据发送给你了，但未必你所有的数据都发送给对方了，所以你未必会马上关闭socket，也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你没有数据发送给对方了，针对每个FIN报文，都需要一次ack报文，故需要四次挥手。<br>四、TCP的状态变迁图<br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_6.jpg" alt="TCP连接的建立和终止_6"><br><img src="http://7xract.com1.z0.glb.clouddn.com/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E7%BB%88%E6%AD%A2_7.jpg" alt="TCP连接的建立和终止_7"><br>几个状态解析：<br>1)TIME_WAIT状态<br>这种状态也称为2MSL等待状态，MSL即一个报文段的最长生存时间，也就是报文在网络中被丢弃前的最长时间。那么为什么需要等待2倍的MSL呢？这是因为在TIME_WAIT状态之后，需要执行主动关闭，发送ACK，同时还需要加上一倍的MSL，为了等待对方的反馈结果（是否收到重发的FIN），这是因为再发送ACK之后，可能因为诸多原因而导致ACK发送失败，此时Server端会在此发送FIN。<br>正常情况下，client在2MSL期间，对应的socket是不能再被使用的，但是在具体的实现中（如伯克利），则可以通过SO_REUSEADDR选项重用此接口。<br>2)FIN_WAIT_2状态<br>当对方对自己发送的FIN进行了确认，此时将进入FIN_WAIT_2状态。<br>3) CLOSE_WAIT状态与FIN_WAIT_1状态<br>当连接中的一方收到对方发过来的FIN时，它将进入CLOSE_WAIT状态，而另一端则进入FIN_WAIT_1状态。</p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/network/">network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-VLAN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/02/20/VLAN/" class="article-date">
  	<time datetime="2015-02-20T02:13:09.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/VLAN/">VLAN</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="VLAN"><a href="#VLAN" class="headerlink" title="VLAN"></a>VLAN</h2><blockquote>
<p>概念：VLAN（Virtual Local Area Network）<br>作用：隔离业务；分割广播域<br>默认情况下，交换机自带VLAN1，所有的端口都属于VLAN1。<br><img src="http://7xract.com1.z0.glb.clouddn.com/3_VLAN1.png" alt="VLAN隔离业务"><br>如：192.168.10.0/24 192.168.20.0/24<br><img src="http://7xract.com1.z0.glb.clouddn.com/3_VLAN2.png" alt="VLAN分割广播域"><br><img src="http://7xract.com1.z0.glb.clouddn.com/3_VLAN3.png" alt="Dot1Q以太网帧格式"><br>VLAN的区分：<br>    在源地址和长度/类型字节之间插入长度为4字节的802.1Q帧头。<br><img src="http://7xract.com1.z0.glb.clouddn.com/3_VLAN4.png" alt="Dot1Q帧抓包"><br><img src="http://7xract.com1.z0.glb.clouddn.com/3_VLAN5.png" alt="相同VLAN可以互访"><br><a href="http://edu.51cto.com/lesson/id-20980.html" target="_blank" rel="external">“VLAN”视频播放地址</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/network/">network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Route" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/02/19/Route/" class="article-date">
  	<time datetime="2015-02-19T13:05:19.000Z" itemprop="datePublished">2015-02-19</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/19/Route/">路由器工作原理及数据流分析</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="u8DEF_u7531_u5668_u5DE5_u4F5C_u539F_u7406_u53CA_u6570_u636E_u6D41_u5206_u6790"><a href="#u8DEF_u7531_u5668_u5DE5_u4F5C_u539F_u7406_u53CA_u6570_u636E_u6D41_u5206_u6790" class="headerlink" title="路由器工作原理及数据流分析"></a>路由器工作原理及数据流分析</h2><blockquote>
<p><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route1.png" alt="路由器工作原理"><br>路由表：网段+掩码 出口<br>    源主机将目标主机的IP地址与自己的掩码进行“与”运算，同时将自己的IP地址也与自己掩码“与”，如相同，则在同一网段。<br>当同一网段时：通过ARP（Address Resolution Protocol）地址解析协议，是根据IP地址获取物理地址的一个TCP/IP协议，请求对方MAC，直接进行二层通信。<br>当不在同网段时，将数据包发送给默认网关，由网关进行三层转发。<br>“与”运算时用的是自己的掩码。如下图所示：<br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route2.png" alt="主机通信原理测试"></p>
</blockquote>
<hr>
<blockquote>
<p>三层转发过程：<br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route3.png" alt="三层转发过程"><br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route4.png" alt="步骤一"><br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route5.png" alt="步骤二"><br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route6.png" alt="步骤三"><br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route7.png" alt="步骤四"><br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route8.png" alt="步骤五"><br>TTL是 Time To Live的缩写，该字段指定IP包被路由器丢弃之前允许通过的最大网段数量。TTL是IPv4包头的一个8 bit字段。TTL的作用是限制IP数据包在计算机网络中的存在的时间。TTL的最大值是255，TTL的一个推荐值是64。虽然TTL从字面上翻译，是可以存活的时间，但实际上TTL是IP数据包在计算机网络中可以转发的最大跳数。TL字段由IP数据包的发送者设置，在IP数据包从源到目的的整个转发路径上，每经过一个路由器，路由器都会修改这个TTL字段值，具体的做法是把该TTL的值减1，然后再将IP包转发出去。如果在IP包到达目的IP之前，TTL减少为0，路由器将会丢弃收到的TTL=0的IP包并向IP包的发送者发送 ICMP time exceeded消息。<br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route9.png" alt="步骤六"></p>
</blockquote>
<hr>
<blockquote>
<p>电脑的ARP表：<br><img src="http://7xract.com1.z0.glb.clouddn.com/2_Route10.png" alt="电脑ARP表"><br>可以看到局域网内广播和组播的地址，以及联系过的电脑主机。<br><a href="http://edu.51cto.com/lesson/id-20964.html" target="_blank" rel="external">“路由器工作原理及数据流分析”视频播放地址</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/network/">network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Net&amp;&amp;Switsh" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/01/17/Net&&Switsh/" class="article-date">
  	<time datetime="2015-01-17T08:08:21.000Z" itemprop="datePublished">2015-01-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/17/Net&&Switsh/">网络三要素及交换机工作原理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="u7F51_u7EDC_u4E09_u8981_u7D20_u53CA_u4EA4_u6362_u673A_u5DE5_u4F5C_u539F_u7406"><a href="#u7F51_u7EDC_u4E09_u8981_u7D20_u53CA_u4EA4_u6362_u673A_u5DE5_u4F5C_u539F_u7406" class="headerlink" title="网络三要素及交换机工作原理"></a>网络三要素及交换机工作原理</h2><blockquote>
<p><img src="http://7xract.com1.z0.glb.clouddn.com/1_Net%26%26Switsh1.png" alt="网络三要素"><br>网络三要素：终端、介质、网络设备<br>终端：电脑、手机等。<br>介质：网线、wifi、光纤等。<br>网络设备：路由器、交换机、防火墙、负载均衡设备等。</p>
</blockquote>
<ol>
<li><p>交换机工作原理</p>
<blockquote>
<p>HUB与Switch内部构造对比：<br><img src="http://7xract.com1.z0.glb.clouddn.com/1_Net%26%26Switsh2.png" alt="HUB与Switch内部构造对比"><br>外形相似，HUB利用总线发送数据，直接广播。采用CSMA/CD（Carrier Sense Multiple Access with Collision Detection）即带冲突检测的载波监听多路访问技术。CSMA/CD的工作原理是: 若信道空闲，发送数据。若忙碌，则等待信道数据传输结束后再发送数据；若冲突,则停止发送数据，等待再重新尝试。缺点：效率不高。交换机利用交换矩阵发送数据，根据mac地址发送数据。</p>
</blockquote>
</li>
<li><p>交换机的学习功能</p>
<blockquote>
<p><img src="http://7xract.com1.z0.glb.clouddn.com/1_Net%26%26Switsh1.png" alt="网络三要素"><br>开始交换机中的MAC地址为空。<br>如E0开始传输数据，则MAC地址表写入E0：0260.8C01.1111。即交换机对于收到的帧，学习其源MAC地址放入MAC地址表。<br>接着，交换机根据收到的帧中间的目的MAC地址，查找MAC地址表进行转发：<br>—　如果匹配，则从相应的端口转发出去（除FCS外，不做改动），只改变FCS。<br>侦校验和：<br>Frame Check Sequence:<br>这个字段包括4字节循环冗余校检码(CRC)用于检查错误.当一个原站组装一个 MAC帧，他在所有字节(从Destination MAC Address到Pad字段)执行一个CRC 计算，原站将计算的结果放入这个字段，并作为帧的一部分传输给目的站， 当帧被目的站接受后，目的站进行同样的校检，如果校检和同字段中的值不同，目的站将认为在传输中发生错误并丢弃这个帧.<br>—　如果没有匹配，则泛洪（flooding），除了源端口外向所有端口转发。此时，B、D发现MAC地址不匹配，丢弃数据帧。C接收数据帧，并回复A，交换机学习C的MAC地址。<br>广播帧：目的MAC地址：FFFF.FFFF.FFFF，传给LAN中所有设备。<br>组播帧：通常用于IP组播的以太网的组播地址以0100.5E或0100.5F开头，这些帧发送给LAN中的一组设备，而不是全部。<br><img src="http://7xract.com1.z0.glb.clouddn.com/1_Net%26%26Switsh4.png" alt="组播帧的处理"><br>交换机内部交换方式：<br>直接转发：交换机检测到目标MAC地址后即转发帧。（快，错误率较高）<br>存储转发：收到完整的帧并检查无错误后才转发。（最常用，慢，正确率高）<br>无碎片转发：交换机检测到帧的前64字节（实验得出）后即转发。（折中）<br><a href="http://edu.51cto.com/lesson/id-20656.html" target="_blank" rel="external">“网络三要素及交换机工作原理”视频播放地址</a></p>
</blockquote>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/network/">network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Oylwhu Blog
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>